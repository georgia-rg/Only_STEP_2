{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65989439-8dbf-4854-b416-716454467da2",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc6c5f9e-3341-47e6-88ec-6b9efad67b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:18:18.415658500Z",
     "start_time": "2026-02-10T19:18:18.275177300Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.dates import DateFormatter\n",
    "import glob\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LogNorm, LinearSegmentedColormap\n",
    "from datetime import timedelta\n",
    "from matplotlib.ticker import ScalarFormatter, LogLocator\n",
    "from matplotlib.ticker import FuncFormatter, LogLocator\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import FuncFormatter, LogFormatter\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pytz\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "from pptx import Presentation\n",
    "from matplotlib.patches import Patch\n",
    "from pptx.util import Inches\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from functools import reduce"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b1bcc5db-2ff7-4fc5-9a5f-3f54c868841f",
   "metadata": {},
   "source": [
    "# Set Plotting Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198128d0-ef88-4d4f-b023-085ea113e780",
   "metadata": {},
   "source": [
    "Fontsize, linewidth, etc. Feel free to change these as you wish"
   ]
  },
  {
   "cell_type": "code",
   "id": "42a97ab1-eada-4e22-b5a0-65262f2caab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:18:21.859905200Z",
     "start_time": "2026-02-10T19:18:21.835038700Z"
    }
   },
   "source": [
    "np.set_printoptions(precision = 4)\n",
    "\n",
    "mpl.rcParams['xtick.direction'] = 'in'\n",
    "mpl.rcParams['ytick.direction'] = 'in'\n",
    "mpl.rcParams['xtick.major.width'] = '1.5'\n",
    "mpl.rcParams['ytick.major.width'] = '1.5'\n",
    "mpl.rcParams['xtick.minor.width'] = '1.0'\n",
    "mpl.rcParams['xtick.minor.width'] = '1.0'\n",
    "mpl.rcParams['xtick.major.size'] = '10.0'\n",
    "mpl.rcParams['ytick.major.size'] = '10.0'\n",
    "mpl.rcParams['xtick.minor.size'] = '5.0'\n",
    "mpl.rcParams['ytick.minor.size'] = '5.0'\n",
    "mpl.rcParams['axes.linewidth'] = '1.5'\n",
    "mpl.rcParams['axes.labelsize'] = '20'\n",
    "mpl.rcParams['axes.titlesize'] = '20' \n",
    "mpl.rcParams['figure.titlesize'] = '20'\n",
    "mpl.rcParams['xtick.labelsize'] = '20'\n",
    "mpl.rcParams['ytick.labelsize'] = '20'\n",
    "mpl.rcParams['xtick.major.pad']='10'\n",
    "mpl.rcParams['ytick.major.pad']='10'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "164f980d-fa44-4929-98cc-3e5b1326b0a9",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "id": "042bee7e-4189-4e01-b717-f897f514a461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:30:11.331372400Z",
     "start_time": "2026-02-10T19:30:11.296098500Z"
    }
   },
   "source": [
    "# Path to SMPS data file (the cleaned_csv saved at the end of code in cleaning_smps_csv_VIB.ipynb)\n",
    "pathp = 'C:/Users/GeorgiaRg/Documents/ASCENT/SMPS/SMPS_data_analysis_VB/SMPS_Data_Shiny/smps_full_dataset_2023-08-04_to_2025-08-25_GA.csv'\n",
    "\n",
    "# # Path to folder in which you keep your Aethelometer csv files\n",
    "# aeth_path = '/Users/vineblankenship/Research/ASCENT/AETHDataSummer2025'\n",
    "#\n",
    "# # Path to individual Aethelometer csv files for the concatenation step\n",
    "# aeth_csv_files = glob.glob(aeth_path  + '/*.csv')\n",
    "\n",
    "# output_folder is the destination folder of all of the plots below\n",
    "output_folder = 'C:/Users/GeorgiaRg/Documents/ASCENT/SMPS/SMPS_data_analysis_VB/SMPS_Data_Shiny/Output'\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "fad74b46-724e-4158-8b65-01a4b49ab2d0",
   "metadata": {},
   "source": [
    "# Define the dataframe as dfp (short for DataFrame Pico Rivera, in this case)\n",
    "\n",
    "Load in the csv from the SMPS path defined above and define a time series dataframe for which the local_time column is the index"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:32:59.939748800Z",
     "start_time": "2026-02-10T19:32:55.202072400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfp = pd.read_csv(pathp)\n",
    "dfp['local_time'] = pd.to_datetime(dfp['local_time'])\n",
    "print(dfp.head())"
   ],
   "id": "17cb1ea2c642e007",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           local_time  Scan Number  Communication Status  \\\n",
      "0 2023-08-04 12:17:00          1.0                  0.01   \n",
      "1 2023-08-04 12:21:00          2.0                  0.01   \n",
      "2 2023-08-04 12:24:00          NaN                   NaN   \n",
      "3 2023-08-04 12:27:00          1.0                  0.01   \n",
      "4 2023-08-04 12:30:00          NaN                   NaN   \n",
      "\n",
      "   Detector Inlet Flow (L/min)  Detector Counting Flow (L/min)  \\\n",
      "0                          0.6                             0.3   \n",
      "1                          0.6                             0.3   \n",
      "2                          NaN                             NaN   \n",
      "3                          0.6                             0.3   \n",
      "4                          NaN                             NaN   \n",
      "\n",
      "   Impactor Flow (L/min)  Impactor D50 (nm)  Sheath Flow (L/min)  \\\n",
      "0                   0.01             1000.0                  4.8   \n",
      "1                   0.01             1000.0                  4.8   \n",
      "2                    NaN                NaN                  NaN   \n",
      "3                   0.01             1000.0                  4.8   \n",
      "4                    NaN                NaN                  NaN   \n",
      "\n",
      "   Sheath Temp (C)  Sheath Pressure (kPa)  ...  572.55  593.52  615.27  637.8  \\\n",
      "0             25.9                  100.3  ...    0.01    0.01    0.01   0.01   \n",
      "1             26.6                  100.3  ...    0.01    0.01    0.01   0.01   \n",
      "2              NaN                    NaN  ...     NaN     NaN     NaN    NaN   \n",
      "3             26.4                  100.3  ...    0.01    0.01    0.01   0.01   \n",
      "4              NaN                    NaN  ...     NaN     NaN     NaN    NaN   \n",
      "\n",
      "   661.17  685.39  710.5  736.53  763.51  791.48  \n",
      "0    0.01    0.01   0.01    0.01    0.01    0.01  \n",
      "1    0.01    0.01   0.01    0.01    0.01    0.01  \n",
      "2     NaN     NaN    NaN     NaN     NaN     NaN  \n",
      "3    0.01    0.01   0.01    0.01    0.01    0.01  \n",
      "4     NaN     NaN    NaN     NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 148 columns]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:04.815083700Z",
     "start_time": "2026-02-10T19:33:04.591895400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tsdfp = dfp.set_index('local_time')\n",
    "tsdfp = tsdfp.sort_index()\n",
    "\n",
    "print(tsdfp.head())"
   ],
   "id": "c1d9a9eedeaf92d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Scan Number  Communication Status  \\\n",
      "local_time                                               \n",
      "2023-08-04 12:17:00          1.0                  0.01   \n",
      "2023-08-04 12:21:00          2.0                  0.01   \n",
      "2023-08-04 12:24:00          NaN                   NaN   \n",
      "2023-08-04 12:27:00          1.0                  0.01   \n",
      "2023-08-04 12:30:00          NaN                   NaN   \n",
      "\n",
      "                     Detector Inlet Flow (L/min)  \\\n",
      "local_time                                         \n",
      "2023-08-04 12:17:00                          0.6   \n",
      "2023-08-04 12:21:00                          0.6   \n",
      "2023-08-04 12:24:00                          NaN   \n",
      "2023-08-04 12:27:00                          0.6   \n",
      "2023-08-04 12:30:00                          NaN   \n",
      "\n",
      "                     Detector Counting Flow (L/min)  Impactor Flow (L/min)  \\\n",
      "local_time                                                                   \n",
      "2023-08-04 12:17:00                             0.3                   0.01   \n",
      "2023-08-04 12:21:00                             0.3                   0.01   \n",
      "2023-08-04 12:24:00                             NaN                    NaN   \n",
      "2023-08-04 12:27:00                             0.3                   0.01   \n",
      "2023-08-04 12:30:00                             NaN                    NaN   \n",
      "\n",
      "                     Impactor D50 (nm)  Sheath Flow (L/min)  Sheath Temp (C)  \\\n",
      "local_time                                                                     \n",
      "2023-08-04 12:17:00             1000.0                  4.8             25.9   \n",
      "2023-08-04 12:21:00             1000.0                  4.8             26.6   \n",
      "2023-08-04 12:24:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:27:00             1000.0                  4.8             26.4   \n",
      "2023-08-04 12:30:00                NaN                  NaN              NaN   \n",
      "\n",
      "                     Sheath Pressure (kPa)  Sheath Relative Humidity (%)  ...  \\\n",
      "local_time                                                                ...   \n",
      "2023-08-04 12:17:00                  100.3                          17.9  ...   \n",
      "2023-08-04 12:21:00                  100.3                          17.9  ...   \n",
      "2023-08-04 12:24:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:27:00                  100.3                          18.2  ...   \n",
      "2023-08-04 12:30:00                    NaN                           NaN  ...   \n",
      "\n",
      "                     572.55  593.52  615.27  637.8  661.17  685.39  710.5  \\\n",
      "local_time                                                                  \n",
      "2023-08-04 12:17:00    0.01    0.01    0.01   0.01    0.01    0.01   0.01   \n",
      "2023-08-04 12:21:00    0.01    0.01    0.01   0.01    0.01    0.01   0.01   \n",
      "2023-08-04 12:24:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:27:00    0.01    0.01    0.01   0.01    0.01    0.01   0.01   \n",
      "2023-08-04 12:30:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "\n",
      "                     736.53  763.51  791.48  \n",
      "local_time                                   \n",
      "2023-08-04 12:17:00    0.01    0.01    0.01  \n",
      "2023-08-04 12:21:00    0.01    0.01    0.01  \n",
      "2023-08-04 12:24:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:27:00    0.01    0.01    0.01  \n",
      "2023-08-04 12:30:00     NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 147 columns]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paths for AQMD data",
   "id": "56856e81f6c7791d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define specific paths for each parameter\n",
    "# Units: Temp (F), WD (degrees), WS (mph), NO2 (ppb), CO (ppm), O3 (ppb)\n",
    "path_dict = {\n",
    "    'Temp': r'C:\\Users\\GeorgiaRg\\Documents\\ASCENT\\AQMD_Data\\Temperature',\n",
    "    'WD':   r'C:\\Users\\GeorgiaRg\\Documents\\ASCENT\\AQMD_Data\\WD',\n",
    "    'WS':   r'C:\\Users\\GeorgiaRg\\Documents\\ASCENT\\AQMD_Data\\WS',\n",
    "    'NO2':  r'C:\\Users\\GeorgiaRg\\Documents\\ASCENT\\AQMD_Data\\NO2',\n",
    "    'CO':   r'C:\\Users\\GeorgiaRg\\Documents\\ASCENT\\AQMD_Data\\CO',\n",
    "    'O3':   r'C:\\Users\\GeorgiaRg\\Documents\\ASCENT\\AQMD_Data\\O3'\n",
    "}\n",
    "\n",
    "data_frames = []\n",
    "\n",
    "# Loop through each parameter\n",
    "for param_name, folder_path in path_dict.items():\n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"Warning: No CSV files found for {param_name} in {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Read and combine all CSVs for this specific parameter\n",
    "    df_temp = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "    # Check if required columns exist\n",
    "    if 'Date Time' in df_temp.columns and 'Value' in df_temp.columns:\n",
    "        # Convert time column\n",
    "        df_temp['Date Time'] = pd.to_datetime(df_temp['Date Time'])\n",
    "\n",
    "        # --- FILTER STEP ---\n",
    "        # Keep ONLY 'Date Time' and 'Value'\n",
    "        df_temp = df_temp[['Date Time', 'Value']]\n",
    "\n",
    "        # --- RENAME STEP ---\n",
    "        # Rename 'Value' to e.g., 'Temp_Value', 'WS_Value' to avoid conflicts\n",
    "        df_temp = df_temp.rename(columns={'Value': f'{param_name}'})\n",
    "\n",
    "        # Sort and remove duplicate timestamps\n",
    "        df_temp = df_temp.sort_values('Date Time').drop_duplicates(subset=['Date Time'])\n",
    "\n",
    "        data_frames.append(df_temp)\n",
    "    else:\n",
    "        print(f\"Error: Missing 'Date Time' or 'Value' column in {param_name} files.\")\n",
    "\n",
    "# 3. Merge all DataFrames into one\n",
    "\n",
    "if data_frames:\n",
    "    # Merge all dataframes on 'Date Time' using an outer join\n",
    "    df_met_data = reduce(lambda left, right: pd.merge(left, right, on='Date Time', how='outer'), data_frames)\n",
    "\n",
    "    # Sort final dataframe by time\n",
    "    df_met_data = df_met_data.sort_values('Date Time').reset_index(drop=True)\n",
    "\n",
    "    # --- Convert Fahrenheit to Celsius and add new column---\n",
    "    if 'Temp' in df_met_data.columns:\n",
    "        # Create a new column 'Temp_C'\n",
    "        df_met_data['Temp_C'] = (df_met_data['Temp'] - 32) * 5/9\n",
    "\n",
    "        # Optional: Round to 2 decimal places for cleaner data\n",
    "        df_met_data['Temp_C'] = df_met_data['Temp_C'].round(2)\n",
    "\n",
    "    # Verify the import\n",
    "    print(\"Final Combined Data Shape:\", df_met_data.shape)\n",
    "    print(\"Columns:\", df_met_data.columns.tolist())\n",
    "    print(df_met_data.head()) # Preview\n",
    "else:\n",
    "    print(\"No data was imported.\")"
   ],
   "id": "f81d2ce963ab3320",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11ef1067-e499-404a-8a6a-a0c7411487ec",
   "metadata": {},
   "source": "## Locate Relevant Concentration Columns (This varies site to site and it is different for Shiny csv files versus csv files downloaded locally at sites)"
  },
  {
   "cell_type": "code",
   "id": "dfba77d0-4771-4f75-b65a-8d34d6f66f25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:16.989183100Z",
     "start_time": "2026-02-10T19:33:16.814494Z"
    }
   },
   "source": [
    "tsdfp.columns[41:156] # your column range here"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['18.11', '18.77', '19.46', '20.17', '20.91', '21.67', '22.47', '23.29',\n",
       "       '24.14', '25.03',\n",
       "       ...\n",
       "       '572.55', '593.52', '615.27', '637.8', '661.17', '685.39', '710.5',\n",
       "       '736.53', '763.51', '791.48'],\n",
       "      dtype='str', length=106)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "58b09136-42db-4ccc-bcef-de67f2c3c0f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:18.169074600Z",
     "start_time": "2026-02-10T19:33:18.139544800Z"
    }
   },
   "source": [
    "## Define the concentration_columns range as the range you have selected above\n",
    "concentration_columns = slice(41, 156) # your column range here"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "099f3f56-b0ef-41d7-b6af-dde913d2a603",
   "metadata": {},
   "source": [
    "# Calculations from original ASCENT SMPS code: \n",
    "\n",
    "Calculating dN/dlogDp at standard temperature and pressure (STP) "
   ]
  },
  {
   "cell_type": "code",
   "id": "2df01fd2-6109-48f5-a771-0e90ceb40a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:21.213855100Z",
     "start_time": "2026-02-10T19:33:21.186650600Z"
    }
   },
   "source": [
    "# read diameter as array\n",
    "mid_Dp = np.array([float(x) for x in tsdfp.columns[concentration_columns]]) # pico\n",
    "avg_diffp = np.mean(np.diff(np.log10(mid_Dp)))\n",
    "\n",
    "D_boundp = np.full(mid_Dp.shape[0]+1, np.nan)\n",
    "for i in range (1, (len(D_boundp)-1)):\n",
    "    D_boundp[i] = 10 ** (0.5 * (np.log10(mid_Dp[i])+np.log10(mid_Dp[i-1])))\n",
    "    \n",
    "D_boundp[0] = 10 ** (np.log10(mid_Dp[0]) - 0.5*avg_diffp)\n",
    "D_boundp[-1] = 10 ** (np.log10(mid_Dp[-1]) + 0.5*avg_diffp)\n",
    "D_lowp = D_boundp[0:-1]\n",
    "D_highp = D_boundp[1:]\n",
    "dlogDpp = np.log10(D_highp) - np.log10(D_lowp)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean up of bad data_added by GA",
   "id": "73a736842034f63d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:37.940795100Z",
     "start_time": "2026-02-10T19:33:29.817089100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Select only the diameter columns using the slice defined in Cell 9\n",
    "data_cols = tsdfp.iloc[:, concentration_columns]\n",
    "\n",
    "# 2. Identify rows where the number of unique values across these columns is 1\n",
    "# (This catches rows where every bin has the same number, e.g., all 0s or all -999s)\n",
    "bad_rows_mask = data_cols.nunique(axis=1) == 1\n",
    "\n",
    "# 3. Filter the dataframe to keep only the rows that are NOT 'bad'\n",
    "tsdfp_cleaned = tsdfp[~bad_rows_mask].copy()\n",
    "\n",
    "# 4. Print removal statistics for verification\n",
    "n_removed = bad_rows_mask.sum()\n",
    "print(f\"Rows removed: {n_removed}\")\n",
    "print(f\"Original shape: {tsdfp.shape}\")\n",
    "print(f\"Cleaned shape:  {tsdfp_cleaned.shape}\")\n",
    "print(tsdfp_cleaned.head())\n",
    "\n",
    "# 5. Overwrite the main dataframe with the cleaned version for subsequent steps\n",
    "tsdfp = tsdfp_cleaned\n",
    "print(tsdfp.head())"
   ],
   "id": "e80d9ae5be7372ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 596\n",
      "Original shape: (431271, 147)\n",
      "Cleaned shape:  (430675, 147)\n",
      "                     Scan Number  Communication Status  \\\n",
      "local_time                                               \n",
      "2023-08-04 12:24:00          NaN                   NaN   \n",
      "2023-08-04 12:30:00          NaN                   NaN   \n",
      "2023-08-04 12:32:00          NaN                   NaN   \n",
      "2023-08-04 12:35:00          NaN                   NaN   \n",
      "2023-08-04 12:37:00          NaN                   NaN   \n",
      "\n",
      "                     Detector Inlet Flow (L/min)  \\\n",
      "local_time                                         \n",
      "2023-08-04 12:24:00                          NaN   \n",
      "2023-08-04 12:30:00                          NaN   \n",
      "2023-08-04 12:32:00                          NaN   \n",
      "2023-08-04 12:35:00                          NaN   \n",
      "2023-08-04 12:37:00                          NaN   \n",
      "\n",
      "                     Detector Counting Flow (L/min)  Impactor Flow (L/min)  \\\n",
      "local_time                                                                   \n",
      "2023-08-04 12:24:00                             NaN                    NaN   \n",
      "2023-08-04 12:30:00                             NaN                    NaN   \n",
      "2023-08-04 12:32:00                             NaN                    NaN   \n",
      "2023-08-04 12:35:00                             NaN                    NaN   \n",
      "2023-08-04 12:37:00                             NaN                    NaN   \n",
      "\n",
      "                     Impactor D50 (nm)  Sheath Flow (L/min)  Sheath Temp (C)  \\\n",
      "local_time                                                                     \n",
      "2023-08-04 12:24:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:30:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:32:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:35:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:37:00                NaN                  NaN              NaN   \n",
      "\n",
      "                     Sheath Pressure (kPa)  Sheath Relative Humidity (%)  ...  \\\n",
      "local_time                                                                ...   \n",
      "2023-08-04 12:24:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:30:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:32:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:35:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:37:00                    NaN                           NaN  ...   \n",
      "\n",
      "                     572.55  593.52  615.27  637.8  661.17  685.39  710.5  \\\n",
      "local_time                                                                  \n",
      "2023-08-04 12:24:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:30:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:32:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:35:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:37:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "\n",
      "                     736.53  763.51  791.48  \n",
      "local_time                                   \n",
      "2023-08-04 12:24:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:30:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:32:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:35:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:37:00     NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 147 columns]\n",
      "                     Scan Number  Communication Status  \\\n",
      "local_time                                               \n",
      "2023-08-04 12:24:00          NaN                   NaN   \n",
      "2023-08-04 12:30:00          NaN                   NaN   \n",
      "2023-08-04 12:32:00          NaN                   NaN   \n",
      "2023-08-04 12:35:00          NaN                   NaN   \n",
      "2023-08-04 12:37:00          NaN                   NaN   \n",
      "\n",
      "                     Detector Inlet Flow (L/min)  \\\n",
      "local_time                                         \n",
      "2023-08-04 12:24:00                          NaN   \n",
      "2023-08-04 12:30:00                          NaN   \n",
      "2023-08-04 12:32:00                          NaN   \n",
      "2023-08-04 12:35:00                          NaN   \n",
      "2023-08-04 12:37:00                          NaN   \n",
      "\n",
      "                     Detector Counting Flow (L/min)  Impactor Flow (L/min)  \\\n",
      "local_time                                                                   \n",
      "2023-08-04 12:24:00                             NaN                    NaN   \n",
      "2023-08-04 12:30:00                             NaN                    NaN   \n",
      "2023-08-04 12:32:00                             NaN                    NaN   \n",
      "2023-08-04 12:35:00                             NaN                    NaN   \n",
      "2023-08-04 12:37:00                             NaN                    NaN   \n",
      "\n",
      "                     Impactor D50 (nm)  Sheath Flow (L/min)  Sheath Temp (C)  \\\n",
      "local_time                                                                     \n",
      "2023-08-04 12:24:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:30:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:32:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:35:00                NaN                  NaN              NaN   \n",
      "2023-08-04 12:37:00                NaN                  NaN              NaN   \n",
      "\n",
      "                     Sheath Pressure (kPa)  Sheath Relative Humidity (%)  ...  \\\n",
      "local_time                                                                ...   \n",
      "2023-08-04 12:24:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:30:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:32:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:35:00                    NaN                           NaN  ...   \n",
      "2023-08-04 12:37:00                    NaN                           NaN  ...   \n",
      "\n",
      "                     572.55  593.52  615.27  637.8  661.17  685.39  710.5  \\\n",
      "local_time                                                                  \n",
      "2023-08-04 12:24:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:30:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:32:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:35:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "2023-08-04 12:37:00     NaN     NaN     NaN    NaN     NaN     NaN    NaN   \n",
      "\n",
      "                     736.53  763.51  791.48  \n",
      "local_time                                   \n",
      "2023-08-04 12:24:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:30:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:32:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:35:00     NaN     NaN     NaN  \n",
      "2023-08-04 12:37:00     NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 147 columns]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "45742b0b-4182-4e68-8109-1f291fde12f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:49.981678300Z",
     "start_time": "2026-02-10T19:33:49.855656400Z"
    }
   },
   "source": [
    "# STP conversion factor\n",
    "STP_factorp = (101.35/tsdfp['Sheath Pressure (kPa)']) * ((273.15+tsdfp['Sheath Temp (C)'])/273.15)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "62907be3-8760-4ed4-baac-df516c2d84de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:51.764374200Z",
     "start_time": "2026-02-10T19:33:51.728677600Z"
    }
   },
   "source": [
    "# Check the column indices to ensure you run the calculations on the concentrations \n",
    "tsdfp.columns[concentration_columns]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['18.11', '18.77', '19.46', '20.17', '20.91', '21.67', '22.47', '23.29',\n",
       "       '24.14', '25.03',\n",
       "       ...\n",
       "       '572.55', '593.52', '615.27', '637.8', '661.17', '685.39', '710.5',\n",
       "       '736.53', '763.51', '791.48'],\n",
       "      dtype='str', length=106)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Puru's flagged data",
   "id": "4785b2df7b18f72f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:33:55.595329200Z",
     "start_time": "2026-02-10T19:33:53.401421900Z"
    }
   },
   "cell_type": "code",
   "source": "df_flagged = pd.read_csv(r'C:\\Users\\GeorgiaRg\\Documents\\ASCENT\\SMPS\\Cleaned from Puru\\valid_auto_qc_SMPS.csv', dtype={'flag': float, 'comment': str, 'Comments': str, 'manual_qc_flag': float})",
   "id": "c4865eba9f1b8dd2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:34:00.939166400Z",
     "start_time": "2026-02-10T19:33:59.688424400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to Pacific Time (local time of the site)\n",
    "df_flagged['sample_datetime_utc'] = pd.to_datetime(df_flagged['sample_datetime_utc'])\n",
    "# We check if it already has a timezone. If not, we set it to UTC first, then convert.\n",
    "if df_flagged['sample_datetime_utc'].dt.tz is None:\n",
    "    df_flagged['local_time'] = df_flagged['sample_datetime_utc'].dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n",
    "else:\n",
    "    df_flagged['local_time'] = df_flagged['sample_datetime_utc'].dt.tz_convert('US/Pacific')\n",
    "\n",
    "# Use local time as the index for tsdf_flagged\n",
    "tsdf_flagged = df_flagged.set_index('local_time')\n",
    "tsdf_flagged = tsdf_flagged.sort_index()\n",
    "\n",
    "\n",
    "\n",
    "print(df_flagged.head())"
   ],
   "id": "1dc1ca4936af3e43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   site_number   site_code sample_datetime_utc  scan_number  \\\n",
      "0            3  LosAngeles 2023-07-12 20:42:40            1   \n",
      "1            3  LosAngeles 2023-08-04 20:16:39            1   \n",
      "2            3  LosAngeles 2023-08-04 20:19:10            2   \n",
      "3            3  LosAngeles 2023-08-04 20:21:58            3   \n",
      "4            3  LosAngeles 2023-08-04 20:24:29            4   \n",
      "\n",
      "                     test_name detector_status classifier_errors  \\\n",
      "0  ascent_picorivera_2023_180s     Normal Scan       Normal Scan   \n",
      "1          ASCENT_2023_LA_150s     Normal Scan       Normal Scan   \n",
      "2          ASCENT_2023_LA_150s     Normal Scan       Normal Scan   \n",
      "3          ASCENT_2023_LA_150s     Normal Scan       Normal Scan   \n",
      "4          ASCENT_2023_LA_150s     Normal Scan       Normal Scan   \n",
      "\n",
      "   communication_status neutralizer_status  detector_inlet_flow_L_min  ...  \\\n",
      "0                     0                 ON                        0.6  ...   \n",
      "1                     0                 ON                        0.6  ...   \n",
      "2                     0                 ON                        0.6  ...   \n",
      "3                     0                 ON                        0.6  ...   \n",
      "4                     0                 ON                        0.6  ...   \n",
      "\n",
      "                                  concentration_json  raw_concentration_json  \\\n",
      "0  {\"13.10\": 6416.103, \"13.58\": 7994.119, \"14.07\"...                     NaN   \n",
      "1  {\"13.10\": 2229.573, \"13.58\": 3073.388, \"14.07\"...                     NaN   \n",
      "2  {\"13.10\": 2042.243, \"13.58\": 2041.064, \"14.07\"...                     NaN   \n",
      "3  {\"13.10\": 1739.063, \"13.58\": 1275.303, \"14.07\"...                     NaN   \n",
      "4  {\"13.10\": 2473.563, \"13.58\": 2287.337, \"14.07\"...                     NaN   \n",
      "\n",
      "   sample_analysis_id  site_record_id  qc_outcome  flag  comment  \\\n",
      "0              513500           47618           1   NaN      NaN   \n",
      "1              525620           58572           1   NaN      NaN   \n",
      "2              525621           58573           1   NaN      NaN   \n",
      "3              525622           58574           1   NaN      NaN   \n",
      "4              525623           58575           1   NaN      NaN   \n",
      "\n",
      "   manual_qc_flag  Comments                local_time  \n",
      "0             NaN       NaN 2023-07-12 13:42:40-07:00  \n",
      "1             NaN       NaN 2023-08-04 13:16:39-07:00  \n",
      "2             NaN       NaN 2023-08-04 13:19:10-07:00  \n",
      "3             NaN       NaN 2023-08-04 13:21:58-07:00  \n",
      "4             NaN       NaN 2023-08-04 13:24:29-07:00  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleaning dfp file based on Puru's flagged and cleaned file",
   "id": "934b5f64063989e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:34:24.975706800Z",
     "start_time": "2026-02-10T19:34:23.092096700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. PREPARE DFP (Target Data)\n",
    "# ---------------------------------------------------------\n",
    "# Ensure index is datetime\n",
    "if not isinstance(dfp.index, pd.DatetimeIndex):\n",
    "    dfp.index = pd.to_datetime(dfp.index)\n",
    "\n",
    "# Resample dfp to 15-minute averages\n",
    "dfp_15min = dfp.resample('15min').mean(numeric_only=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. PREPARE DF_FLAGGED (QC Data)\n",
    "# ---------------------------------------------------------\n",
    "# Ensure index is datetime\n",
    "if not isinstance(df_flagged.index, pd.DatetimeIndex):\n",
    "    df_flagged.index = pd.to_datetime(df_flagged.index)\n",
    "\n",
    "# TIMEZONE ALIGNMENT:\n",
    "# Ensure both indices are timezone-naive (or both aware) to allow comparison.\n",
    "# We strip timezones to be safe.\n",
    "if dfp_15min.index.tz is not None:\n",
    "    dfp_15min.index = dfp_15min.index.tz_localize(None)\n",
    "if df_flagged.index.tz is not None:\n",
    "    df_flagged.index = df_flagged.index.tz_localize(None)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. CREATE MASKS (Based on df_flagged)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# A. EXISTENCE MASK\n",
    "# Calculate which 15-min bins actually contain data in df_flagged.\n",
    "# count() returns the number of non-NaN items. If > 0, data exists.\n",
    "existence_counts = df_flagged.iloc[:, 0].resample('15min').count()\n",
    "existence_mask = existence_counts > 0\n",
    "\n",
    "# B. FLAG MASK\n",
    "# Define columns to check\n",
    "flag_cols_to_check = ['flag', 'comment', 'Comments', 'manual_qc_flag']\n",
    "valid_flag_cols = [c for c in flag_cols_to_check if c in df_flagged.columns]\n",
    "\n",
    "# Step 1: Identify flagged rows in the raw data (True if any column is not NaN)\n",
    "raw_is_flagged = df_flagged[valid_flag_cols].notna().any(axis=1)\n",
    "\n",
    "# Step 2: Resample to 15min to find flagged windows.\n",
    "# FIX for AttributeError: Convert True/False to 1/0, then take the max().\n",
    "# If max is 1, it means at least one row in that 15min window was flagged.\n",
    "flag_mask_numeric = raw_is_flagged.astype(int).resample('15min').max()\n",
    "flag_mask = (flag_mask_numeric == 1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. APPLY CLEANING TO DFP\n",
    "# ---------------------------------------------------------\n",
    "# We want to keep rows where:\n",
    "# 1. Data Exists in df_flagged (existence_mask is True)\n",
    "# 2. Data is NOT Flagged (flag_mask is False)\n",
    "\n",
    "# Align the masks to the dfp index (in case dfp has wider range)\n",
    "# reindex ensures the series matches dfp_15min, filling missing spots with defaults\n",
    "aligned_existence = existence_mask.reindex(dfp_15min.index, fill_value=False)\n",
    "aligned_flags = flag_mask.reindex(dfp_15min.index, fill_value=False)\n",
    "\n",
    "# Filter: Keep if (Exists in QC) AND (Not Flagged)\n",
    "clean_condition = aligned_existence & (~aligned_flags)\n",
    "\n",
    "# Apply the filter\n",
    "dfp_15min_cleaned = dfp_15min.loc[clean_condition]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. OUTPUT\n",
    "# ---------------------------------------------------------\n",
    "print(f\"Original dfp_15min rows: {len(dfp_15min)}\")\n",
    "print(f\"Cleaned dfp_15min rows:  {len(dfp_15min_cleaned)}\")\n",
    "print(dfp_15min_cleaned.head())"
   ],
   "id": "f241e5bcee1deadb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dfp_15min rows: 1\n",
      "Cleaned dfp_15min rows:  0\n",
      "Empty DataFrame\n",
      "Columns: [Scan Number, Communication Status, Detector Inlet Flow (L/min), Detector Counting Flow (L/min), Impactor Flow (L/min), Impactor D50 (nm), Sheath Flow (L/min), Sheath Temp (C), Sheath Pressure (kPa), Sheath Relative Humidity (%), Mean Free Path (m), Gas Viscosity (Pa*s), Low Voltage (V), High Voltage (V), Lower Size (nm), Upper Size (nm), Wide Range Scan Mode, DMA V Ramping Up (TUP) (s), DMA V Ramping Down (TDOWN) (s), DMA Column transit time Tf (s), DMA Exit to Optical Detector Td (s), DMA at Low Voltage (TLOW) (s), DMA at High Voltage (THIGH) (s), Adjustment (s), Dilution Factor, Aerosol Density (g/cm³), Median (nm), Mean (nm), Geo. Mean (nm), Mode (nm), Geo. Std. Dev, Total Concentration (#/cm³), 13.1, 13.58, 14.07, 14.59, 15.12, 15.68, 16.25, 16.85, 17.47, 18.11, 18.77, 19.46, 20.17, 20.91, 21.67, 22.47, 23.29, 24.14, 25.03, 25.95, 26.9, 27.88, 28.9, 29.96, 31.06, 32.2, 33.38, 34.6, 35.87, 37.18, 38.54, 39.95, 41.42, 42.94, 44.51, 46.14, 47.83, 49.58, 51.4, 53.28, 55.23, 57.25, 59.35, 61.53, 63.78, 66.12, 68.54, 71.05, 73.65, 76.35, 79.15, 82.05, 85.05, 88.17, 91.4, 94.75, 98.22, 101.82, 105.54, 109.41, 113.42, 117.57, 121.88, 126.35, 130.97, 135.77, 140.75, 145.9, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 147 columns]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#To be able to use this cleaned from Puru version for the rest of the code, you have to do this small cell\n",
    "tsdfp = dfp_15min_cleaned\n",
    "tsdfp.columns[41:156]"
   ],
   "id": "48c9793f9c871556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "concentration_columns = slice(41, 156)\n",
    "# read diameter as array\n",
    "mid_Dp = np.array([float(x) for x in tsdfp.columns[concentration_columns]]) # pico\n",
    "avg_diffp = np.mean(np.diff(np.log10(mid_Dp)))\n",
    "\n",
    "D_boundp = np.full(mid_Dp.shape[0]+1, np.nan)\n",
    "for i in range (1, (len(D_boundp)-1)):\n",
    "    D_boundp[i] = 10 ** (0.5 * (np.log10(mid_Dp[i])+np.log10(mid_Dp[i-1])))\n",
    "\n",
    "D_boundp[0] = 10 ** (np.log10(mid_Dp[0]) - 0.5*avg_diffp)\n",
    "D_boundp[-1] = 10 ** (np.log10(mid_Dp[-1]) + 0.5*avg_diffp)\n",
    "D_lowp = D_boundp[0:-1]\n",
    "D_highp = D_boundp[1:]\n",
    "dlogDpp = np.log10(D_highp) - np.log10(D_lowp)\n",
    "STP_factorp = (101.35/tsdfp['Sheath Pressure (kPa)']) * ((273.15+tsdfp['Sheath Temp (C)'])/273.15)\n",
    "tsdfp.columns[concentration_columns]"
   ],
   "id": "ca79a347f486f9a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "217538a2-48dc-4405-a5d2-b7494c2211b1",
   "metadata": {},
   "source": [
    "## Define dNdlogDp_stpp, dVdlogDp_stpp, dMdlogDp_stpp, N_stp, V_stp, M_stp"
   ]
  },
  {
   "cell_type": "code",
   "id": "46401788-8684-4969-9e81-81cf90a92003",
   "metadata": {},
   "source": [
    "# calculate total number of each scan \n",
    "artsdfp = np.array(tsdfp)\n",
    "\n",
    "dNdlogDpp = artsdfp[:, concentration_columns] # pico\n",
    "\n",
    "dNdlogDp_stpp = dNdlogDpp * np.array(STP_factorp)[:,None]\n",
    "\n",
    "dN_stpp = dNdlogDp_stpp * dlogDpp\n",
    "\n",
    "N_stp = np.nansum(dN_stpp, axis=1)\n",
    "\n",
    "# # calculate volume distribution and total volume of each scan \n",
    "dVdlogDp_stpp = (np.pi/6.) * (mid_Dp/1e3) **3 * dNdlogDp_stpp  #um3/cm3\n",
    "\n",
    "dV_stpp = dVdlogDp_stpp * dlogDpp\n",
    "\n",
    "V_stp = np.nansum(dV_stpp, axis=1)         #um3/cm3\n",
    "\n",
    "# calculate mass distribution and total mass of each scan by assuming a particle density\n",
    "density = 1.0      #  g/cm³\n",
    "dMdlogDp_stpp = (density/1e9) * (np.pi/6.) * mid_Dp**3 * dNdlogDp_stpp    #ug/m3\n",
    "\n",
    "dM_stp = dMdlogDp_stpp * dlogDpp\n",
    "\n",
    "M_stp = np.nansum(dM_stp, axis=1)         #ug/m3\n",
    "\n",
    "print(dM_stp.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Monthly average dNdlogDp and dMdlogDp (exported images in a powerpoint)",
   "id": "b47fe31ab892b609"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Initialize the PowerPoint Presentation\n",
    "prs = Presentation()\n",
    "\n",
    "# 2. Prepare the data\n",
    "# Ensure the index is datetime\n",
    "if not isinstance(tsdfp.index, pd.DatetimeIndex):\n",
    "    tsdfp.index = pd.to_datetime(tsdfp.index)\n",
    "\n",
    "# Create temporary DataFrames for easier grouping\n",
    "# We assume dNdlogDp_stpp and dMdlogDp_stpp align with tsdfp rows\n",
    "df_dN = pd.DataFrame(dNdlogDp_stpp, index=tsdfp.index)\n",
    "df_dM = pd.DataFrame(dMdlogDp_stpp, index=tsdfp.index)\n",
    "\n",
    "# 3. Group by Month\n",
    "# We use the index of one of them to define the groups\n",
    "monthly_groups = df_dN.groupby(df_dN.index.to_period('M'))\n",
    "\n",
    "print(f\"Generating monthly distribution slides for {len(monthly_groups)} months...\")\n",
    "\n",
    "# 4. Loop through each month\n",
    "for period, group_dN in monthly_groups:\n",
    "    month_name = period.strftime('%B %Y')\n",
    "\n",
    "    # Get the corresponding dM data for this month using the index\n",
    "    group_dM = df_dM.loc[group_dN.index]\n",
    "\n",
    "    # Calculate the mean across time (axis 0) for this month\n",
    "    # This results in a 1D array of average distribution for the month\n",
    "    avg_dN = group_dN.mean(axis=0).values\n",
    "    avg_dM = group_dM.mean(axis=0).values\n",
    "\n",
    "    # --- Create Figure with 2 Subplots (Top: dN, Bottom: dM) ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # ==========================================\n",
    "    # PLOT 1: Average dN/dlogDp (Top)\n",
    "    # ==========================================\n",
    "    # mid_Dp is the diameter array from Cell 8\n",
    "    ax1.plot(mid_Dp, avg_dN, color='tab:blue', linewidth=2)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_ylabel('dN/dlogDp (#/cm³)', fontsize=12)\n",
    "    ax1.set_title(f'{month_name}: Average Number Size Distribution', fontsize=14)\n",
    "    ax1.grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "\n",
    "    # ==========================================\n",
    "    # PLOT 2: Average dM/dlogDp (Bottom)\n",
    "    # ==========================================\n",
    "    ax2.plot(mid_Dp, avg_dM, color='tab:red', linewidth=2)\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_xlabel('Particle Diameter (nm)', fontsize=12)\n",
    "    ax2.set_ylabel('dM/dlogDp (µg/m³)', fontsize=12)\n",
    "    ax2.set_title(f'{month_name}: Average Mass Size Distribution', fontsize=14)\n",
    "    ax2.grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- Save Figure to Memory Buffer ---\n",
    "    image_stream = io.BytesIO()\n",
    "    plt.savefig(image_stream, format='png', dpi=150)\n",
    "    image_stream.seek(0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # --- Add Slide to PowerPoint ---\n",
    "    slide_layout = prs.slide_layouts[6] # Blank slide\n",
    "    slide = prs.slides.add_slide(slide_layout)\n",
    "\n",
    "    # Add a title text box for clarity (Optional)\n",
    "    # txBox = slide.shapes.add_textbox(Inches(0.5), Inches(0.2), Inches(9), Inches(0.5))\n",
    "    # txBox.text_frame.text = f\"Distributions: {month_name}\"\n",
    "\n",
    "    # Center image on slide\n",
    "    pic_height = Inches(7.0)\n",
    "    left = Inches(1.5)\n",
    "    top = Inches(0.25)\n",
    "\n",
    "    slide.shapes.add_picture(image_stream, left, top, height=pic_height)\n",
    "\n",
    "# 5. Save the Presentation\n",
    "output_filename = 'Monthly_Size_Distributions_dN_dM.pptx'\n",
    "output_pptx = os.path.join(output_folder, output_filename)\n",
    "prs.save(output_pptx)\n",
    "\n",
    "print(f\"Presentation saved successfully to: {output_pptx}\")"
   ],
   "id": "6b06663b89732a35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Export dM and dMdlogDp to csv files",
   "id": "12609192cafcbc20"
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "code",
   "source": [
    "# 1. Create a DataFrame for the mass distribution data\n",
    "# We use the time index from tsdfp and the diameters (mid_Dp) as column headers\n",
    "df_dM_stp = pd.DataFrame(dM_stp, index=tsdfp.index, columns=mid_Dp)\n",
    "df_dMdlogDp = pd.DataFrame(dMdlogDp_stpp, index=tsdfp.index, columns=mid_Dp)\n",
    "\n",
    "# 2. Construct the full output filename\n",
    "# This uses the 'output_folder' variable defined earlier in your notebook\n",
    "output_filename = os.path.join(output_folder, 'dMdlogDp_stpp_export.csv')\n",
    "output_filename_dM = os.path.join(output_folder, 'dM_stp_export.csv')\n",
    "\n",
    "# 3. Export to CSV\n",
    "df_dMdlogDp.to_csv(output_filename)\n",
    "df_dM_stp.to_csv(output_filename_dM)"
   ],
   "id": "a0d4adc92676c7da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PM0.1 and PM1 calculations",
   "id": "5177d064ff1dc6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We convert the index to datetime to ensure resampling works, using 'coerce' to handle any bad timestamp strings\n",
    "# tsdfp is from Cell 5\n",
    "timestamps = pd.to_datetime(tsdfp.index, errors='coerce')\n",
    "df_sums = pd.DataFrame(index=timestamps)\n",
    "\n",
    "# Identify indices where the diameter (mid_Dp) is less than 105\n",
    "# mid_Dp is the array of diameters corresponding to the columns of dM_stp\n",
    "indices_lt_105 = np.where(mid_Dp < 105)[0]\n",
    "\n",
    "# Identify indices where the diameter (mid_Dp) is greater than 50 nm for N50\n",
    "indices_gt_50 = np.where(mid_Dp > 50)[0]\n",
    "\n",
    "# Calculate the row-wise sums\n",
    "# Sum of columns where Dp < 105 nm, meaning PM0.1 calculation (ultrafine particles, pm_f)\n",
    "pm_upf = np.nansum(dM_stp[:, indices_lt_105], axis=1)\n",
    "\n",
    "# Sum of all columns, meaning PM1 calculation (fine particles, pm_f)\n",
    "pm_f = np.nansum(dM_stp, axis=1)\n",
    "\n",
    "# Calculate N50: Sum of columns where Dp > 50 nm using dN (dN_stpp)\n",
    "n50_val = np.nansum(dN_stpp[:, indices_gt_50], axis=1)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "# We use the index from tsdfp to ensure the timestamps align correctly\n",
    "df_sums['PM0.1'] = pm_upf\n",
    "df_sums['PM1'] = pm_f\n",
    "df_sums['N10'] = N_stp\n",
    "df_sums['N50'] = n50_val\n",
    "\n",
    "# --- CLEANING STEPS ---\n",
    "\n",
    "# 1. Replace 0 with NaN\n",
    "# This prevents periods with missing data (which sum to 0) from skewing averages\n",
    "df_sums = df_sums.replace(0, np.nan)\n",
    "\n",
    "# 2. Filter out rows where PM1 > 150 (likely outliers/bad data)\n",
    "# We set the entire row to NaN so these scans are not included in the hourly average\n",
    "df_sums.loc[df_sums['PM1'] > 150, :] = np.nan\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "# Resample to Hourly Averages\n",
    "# 'H' stands for hourly frequency. We use mean() to get the average concentration per hour.\n",
    "df_hourly = df_sums.resample('1h').mean()\n",
    "\n",
    "# Drop any empty hours (optional, keeps plot clean)\n",
    "df_hourly = df_hourly.dropna()\n",
    "\n",
    "# Export to CSV\n",
    "output_filename_hourly = os.path.join(output_folder, 'hourly_averages.csv')\n",
    "df_hourly.to_csv(output_filename_hourly)\n",
    "\n",
    "print(df_hourly)"
   ],
   "id": "81256081b7115514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot hourly PM0.1 vs PM1 for the whole data file",
   "id": "edf7aa946a0f8727"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Create the figure and primary axis (left)\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot 'PM0.1' on the left axis (Blue)\n",
    "color_left = 'tab:blue'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('PM0.1', color=color_left, fontsize=12)\n",
    "ax1.plot(df_hourly.index, df_hourly['PM0.1'], color=color_left, linewidth=1, label='PM0.1')\n",
    "ax1.tick_params(axis='y', labelcolor=color_left)\n",
    "\n",
    "# 2. Create the secondary axis (right) that shares the x-axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot 'PM1' on the right axis (Red)\n",
    "color_right = 'tab:red'\n",
    "ax2.set_ylabel('PM1', color=color_right, fontsize=12)\n",
    "ax2.plot(df_hourly.index, df_hourly['PM1'], color=color_right, linewidth=1, linestyle='--', label='PM1')\n",
    "ax2.tick_params(axis='y', labelcolor=color_right)\n",
    "\n",
    "# 3. Formatting the Date Axis\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
    "fig.autofmt_xdate() # Rotation\n",
    "\n",
    "# 4. Title and Layout\n",
    "plt.title('Time Series: PM0.1 vs PM1', fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "id": "524505a63b4ddafd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot hourly PM0.1 vs PM1 and N10 monthly timeseries and scatterplot and export the graphs in a powerpoint",
   "id": "abeac099b5e79325"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================================================\n",
    "# POWERPOINT GENERATION\n",
    "# =========================================================\n",
    "\n",
    "# 1. Check if df_hourly exists (from Cell 26)\n",
    "if 'df_hourly' not in locals():\n",
    "    raise ValueError(\"variable 'df_hourly' not found. Please run the 'PM0.1 and PM1 calculations' cell (Cell 26) first.\")\n",
    "\n",
    "# 2. Initialize Presentation\n",
    "prs = Presentation()\n",
    "\n",
    "# 3. Group data by Month\n",
    "# Ensure index is datetime for grouping\n",
    "if not isinstance(df_hourly.index, pd.DatetimeIndex):\n",
    "    df_hourly.index = pd.to_datetime(df_hourly.index)\n",
    "\n",
    "monthly_groups = df_hourly.groupby(df_hourly.index.to_period('M'))\n",
    "\n",
    "# 4. Helper Function for 3-Panel Slide\n",
    "def create_three_panel_slide(prs, df_data, month_name, col_y1, col_y2, y1_label, y2_label, y2_color):\n",
    "    \"\"\"\n",
    "    Creates a slide with:\n",
    "    - Top: Time Series (spanning width) with VISIBLE timestamps\n",
    "    - Bottom Left: Scatter Plot\n",
    "    - Bottom Right: Diurnal Plot\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Statistics ---\n",
    "    valid_data = df_data[[col_y1, col_y2]].dropna()\n",
    "    if len(valid_data) > 1:\n",
    "        corr = valid_data[col_y1].corr(valid_data[col_y2])\n",
    "        r_squared = corr ** 2\n",
    "    else:\n",
    "        r_squared = 0.0\n",
    "\n",
    "    # Diurnal average (group by hour)\n",
    "    diurnal_df = df_data.groupby(df_data.index.hour)[[col_y1, col_y2]].mean()\n",
    "\n",
    "    # --- 2. Figure Setup ---\n",
    "    fig = plt.figure(figsize=(13, 10))\n",
    "    # Adjust layout to make room for x-axis labels\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[2, 1], hspace=0.4, wspace=0.25)\n",
    "\n",
    "    # --- PLOT 1: Time Series (TOP) ---\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    color_left = 'tab:blue'\n",
    "    ax1.set_xlabel('Date', fontsize=12)\n",
    "    ax1.set_ylabel(y1_label, color=color_left, fontsize=12)\n",
    "    ax1.plot(df_data.index, df_data[col_y1], color=color_left, linewidth=1, label=y1_label)\n",
    "    ax1.tick_params(axis='y', labelcolor=color_left)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(y2_label, color=y2_color, fontsize=12)\n",
    "    ax2.plot(df_data.index, df_data[col_y2], color=y2_color, linewidth=1, linestyle='--', label=y2_label)\n",
    "    ax2.tick_params(axis='y', labelcolor=y2_color)\n",
    "\n",
    "    # --- X-AXIS FORMATTING (Timestamps) ---\n",
    "    # Set tick every 3 days to ensure visibility without overcrowding\n",
    "    ax1.xaxis.set_major_locator(mdates.DayLocator(interval=3))\n",
    "    # Format as Month-Day (e.g., 01-15)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "    # Rotate labels 45 degrees\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    # Force label visibility\n",
    "    ax1.tick_params(axis='x', which='major', labelbottom=True)\n",
    "\n",
    "    ax1.set_title(f'Time Series: {month_name} ({y1_label} vs {y2_label})', fontsize=14)\n",
    "    ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    # --- PLOT 2: Scatter Plot (BOTTOM LEFT) ---\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3.scatter(df_data[col_y1], df_data[col_y2], alpha=0.6, color='purple', edgecolors='w', s=40)\n",
    "    ax3.set_xlabel(f'{y1_label}')\n",
    "    ax3.set_ylabel(f'{y2_label}')\n",
    "    ax3.set_title(f'Correlation ($R^2 = {r_squared:.3f}$)', fontsize=12)\n",
    "    ax3.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- PLOT 3: Diurnal Plot (BOTTOM RIGHT) ---\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    ax4.set_xlabel('Hour of Day')\n",
    "    ax4.set_ylabel(y1_label, color=color_left, fontsize=12)\n",
    "    ax4.plot(diurnal_df.index, diurnal_df[col_y1], color=color_left, linewidth=2, marker='o', markersize=4)\n",
    "    ax4.tick_params(axis='y', labelcolor=color_left)\n",
    "\n",
    "    ax5 = ax4.twinx()\n",
    "    ax5.set_ylabel(y2_label, color=y2_color, fontsize=12)\n",
    "    ax5.plot(diurnal_df.index, diurnal_df[col_y2], color=y2_color, linewidth=2, linestyle='--', marker='s', markersize=4)\n",
    "    ax5.tick_params(axis='y', labelcolor=y2_color)\n",
    "\n",
    "    ax4.set_xticks(range(0, 24, 4))\n",
    "    ax4.set_xlim(0, 23)\n",
    "    ax4.set_title(f'Diurnal Profile', fontsize=12)\n",
    "    ax4.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- Save to Slide ---\n",
    "    image_stream = io.BytesIO()\n",
    "    plt.savefig(image_stream, format='png', dpi=150)\n",
    "    image_stream.seek(0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    slide_layout = prs.slide_layouts[6] # Blank slide\n",
    "    slide = prs.slides.add_slide(slide_layout)\n",
    "\n",
    "    pic_height = Inches(7.0)\n",
    "    # Center on 10-inch slide (approx width 9.1 inches)\n",
    "    left = Inches(0.45)\n",
    "    top = Inches(0.25)\n",
    "\n",
    "    slide.shapes.add_picture(image_stream, left, top, height=pic_height)\n",
    "\n",
    "\n",
    "# 5. Main Loop to Generate Slides\n",
    "print(f\"Generating 3-slide sets for {len(monthly_groups)} months using CLEANED df_hourly...\")\n",
    "\n",
    "for period, df_month in monthly_groups:\n",
    "    month_name = period.strftime('%B %Y')\n",
    "\n",
    "    # Slide 1: PM0.1 vs PM1 (Red)\n",
    "    create_three_panel_slide(\n",
    "        prs, df_month, month_name,\n",
    "        'PM0.1', 'PM1', 'PM0.1', 'PM1', 'tab:red'\n",
    "    )\n",
    "\n",
    "    # Slide 2: PM0.1 vs N10 (Black)\n",
    "    create_three_panel_slide(\n",
    "        prs, df_month, month_name,\n",
    "        'PM0.1', 'N10', 'PM0.1', 'N10', 'black'\n",
    "    )\n",
    "\n",
    "    # Slide 3: PM0.1 vs N50 (Purple)\n",
    "    create_three_panel_slide(\n",
    "        prs, df_month, month_name,\n",
    "        'PM0.1', 'N50', 'PM0.1', 'N50', 'tab:purple'\n",
    "    )\n",
    "\n",
    "# 6. Save the Presentation\n",
    "output_pptx = os.path.join(output_folder, 'Monthly_Analysis_PM_N10_N50.pptx')\n",
    "prs.save(output_pptx)\n",
    "print(f\"Presentation saved successfully to: {output_pptx}\")"
   ],
   "id": "61b1b2deb5d12eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seasonal Boxplots of PM0.1, PM1, N10 and correlations",
   "id": "b5158cd89547a69c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Prepare Data & Define Seasons\n",
    "# ---------------------------------------------------------\n",
    "# Ensure df_hourly exists (from Cell 26) and index is datetime\n",
    "df_seasonal = df_hourly.copy()\n",
    "if not isinstance(df_seasonal.index, pd.DatetimeIndex):\n",
    "    df_seasonal.index = pd.to_datetime(df_seasonal.index)\n",
    "\n",
    "# Function to map month to season\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "df_seasonal['Season'] = df_seasonal.index.month.map(get_season)\n",
    "season_order = ['Autumn', 'Winter', 'Spring', 'Summer']\n",
    "\n",
    "# --- Print Month Counts ---\n",
    "print(\"=\"*40)\n",
    "print(\"DATA AVAILABILITY PER SEASON\")\n",
    "print(\"=\"*40)\n",
    "for season in season_order:\n",
    "    # Filter data for the season\n",
    "    season_data = df_seasonal[df_seasonal['Season'] == season]\n",
    "\n",
    "    # Count unique Year-Month combinations (e.g., Jan 2023 and Jan 2024 count as 2)\n",
    "    num_months = season_data.index.to_period('M').nunique()\n",
    "\n",
    "    # Optional: Get specific month names for verification\n",
    "    # month_names = season_data.index.strftime('%b-%Y').unique().tolist()\n",
    "\n",
    "    print(f\"{season:<10}: {num_months} months of data\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# =========================================================\n",
    "# BOXPLOT 1: Concentrations with Dual Axes (PM0.1 Left, PM1 Right)\n",
    "# =========================================================\n",
    "\n",
    "# Prepare lists of data for plotting\n",
    "data_pm01 = [df_seasonal[df_seasonal['Season'] == s]['PM0.1'].dropna() for s in season_order]\n",
    "data_pm1 = [df_seasonal[df_seasonal['Season'] == s]['PM1'].dropna() for s in season_order]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax2 = ax1.twinx()  # Create secondary y-axis sharing the same x-axis\n",
    "\n",
    "# Define positions\n",
    "indices = np.arange(len(season_order))\n",
    "width = 0.3  # Width of the boxes\n",
    "\n",
    "# Plot PM0.1 on Left Axis (Shifted Left)\n",
    "bp1 = ax1.boxplot(\n",
    "    data_pm01,\n",
    "    positions=indices - width/1.8,\n",
    "    widths=width,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='tab:blue', color='blue', alpha=0.6),\n",
    "    medianprops=dict(color='black'),\n",
    "    whiskerprops=dict(color='blue'),\n",
    "    capprops=dict(color='blue'),\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "# Plot PM1 on Right Axis (Shifted Right)\n",
    "bp2 = ax2.boxplot(\n",
    "    data_pm1,\n",
    "    positions=indices + width/1.8,\n",
    "    widths=width,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='tab:red', color='red', alpha=0.6),\n",
    "    medianprops=dict(color='black'),\n",
    "    whiskerprops=dict(color='red'),\n",
    "    capprops=dict(color='red'),\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "# Formatting Axes\n",
    "ax1.set_ylabel('PM0.1 Concentration (µg/m³)', color='tab:blue', fontsize=14)\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2.set_ylabel('PM1 Concentration (µg/m³)', color='tab:red', fontsize=14)\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax1.set_xticks(indices)\n",
    "ax1.set_xticklabels(season_order, fontsize=12)\n",
    "ax1.set_xlabel('Season', fontsize=14)\n",
    "ax1.set_title('Seasonal Concentrations: PM0.1 vs PM1', fontsize=16)\n",
    "\n",
    "# Custom Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='tab:blue', edgecolor='blue', alpha=0.6, label='PM0.1 (Left Axis)'),\n",
    "    Patch(facecolor='tab:red', edgecolor='red', alpha=0.6, label='PM1 (Right Axis)')\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc='upper center', frameon=False, ncol=2)\n",
    "\n",
    "ax1.grid(True, linestyle=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Plot 1\n",
    "plot1_filename = os.path.join(output_folder, 'Boxplot_Seasonal_Dual_Axis.png')\n",
    "plt.savefig(plot1_filename, dpi=300)\n",
    "plt.show()\n",
    "print(f\"Boxplot 1 saved to: {plot1_filename}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# CALCULATE DAILY CORRELATIONS (R²)\n",
    "# =========================================================\n",
    "daily_r2_list = []\n",
    "\n",
    "for date, day_group in df_seasonal.groupby(df_seasonal.index.date):\n",
    "    # Need sufficient data points (e.g., > 12 hours)\n",
    "    if len(day_group) > 12:\n",
    "        # 1. R2 for PM0.1 vs PM1\n",
    "        data_pm = day_group[['PM0.1', 'PM1']].dropna()\n",
    "        if len(data_pm) > 12:\n",
    "            r_pm = data_pm['PM0.1'].corr(data_pm['PM1'])\n",
    "            r2_pm = r_pm**2\n",
    "        else:\n",
    "            r2_pm = np.nan\n",
    "\n",
    "        # 2. R2 for PM0.1 vs N10\n",
    "        data_n10 = day_group[['PM0.1', 'N10']].dropna()\n",
    "        if len(data_n10) > 12:\n",
    "            r_n10 = data_n10['PM0.1'].corr(data_n10['N10'])\n",
    "            r2_n10 = r_n10**2\n",
    "        else:\n",
    "            r2_n10 = np.nan\n",
    "\n",
    "        # Store results\n",
    "        month = pd.to_datetime(date).month\n",
    "        season = get_season(month)\n",
    "\n",
    "        daily_r2_list.append({\n",
    "            'Date': date,\n",
    "            'Season': season,\n",
    "            'R2_PM_PM': r2_pm,\n",
    "            'R2_PM_N10': r2_n10\n",
    "        })\n",
    "\n",
    "df_r2 = pd.DataFrame(daily_r2_list)\n",
    "\n",
    "# =========================================================\n",
    "# BOXPLOT 2: R² Correlation (PM0.1 vs PM1)\n",
    "# =========================================================\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(\n",
    "    data=df_r2,\n",
    "    x='Season',\n",
    "    y='R2_PM_PM',\n",
    "    order=season_order,\n",
    "    color='tab:purple',\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "plt.title('Seasonal Distribution of Daily R² (PM0.1 vs PM1)', fontsize=16)\n",
    "plt.ylabel('Daily R² Correlation', fontsize=14)\n",
    "plt.xlabel('Season', fontsize=14)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Plot 2\n",
    "plot2_filename = os.path.join(output_folder, 'Boxplot_Seasonal_R2_PM01_PM1.png')\n",
    "plt.savefig(plot2_filename, dpi=300)\n",
    "plt.show()\n",
    "print(f\"Boxplot 2 saved to: {plot2_filename}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# BOXPLOT 3: R² Correlation (PM0.1 vs N10)\n",
    "# =========================================================\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(\n",
    "    data=df_r2,\n",
    "    x='Season',\n",
    "    y='R2_PM_N10',\n",
    "    order=season_order,\n",
    "    color='tab:green',\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "plt.title('Seasonal Distribution of Daily R² (PM0.1 vs N10)', fontsize=16)\n",
    "plt.ylabel('Daily R² Correlation', fontsize=14)\n",
    "plt.xlabel('Season', fontsize=14)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Plot 3\n",
    "plot3_filename = os.path.join(output_folder, 'Boxplot_Seasonal_R2_PM01_N10.png')\n",
    "plt.savefig(plot3_filename, dpi=300)\n",
    "plt.show()\n",
    "print(f\"Boxplot 3 saved to: {plot3_filename}\")"
   ],
   "id": "60959d77a60151b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seasonal Boxplots of AQMD data (Temp, NO2, O3)",
   "id": "e97b30d5a448c255"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Prepare the Data\n",
    "# Ensure df_met_data exists (from your import step)\n",
    "if 'df_met_data' not in locals():\n",
    "    print(\"Error: 'df_met_data' not found. Please run the data import cell first.\")\n",
    "else:\n",
    "    # Create a working copy\n",
    "    df_met_plot = df_met_data.copy()\n",
    "\n",
    "    # Ensure Date Time is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_met_plot['Date Time']):\n",
    "        df_met_plot['Date Time'] = pd.to_datetime(df_met_plot['Date Time'])\n",
    "\n",
    "    # Set index for easy time manipulation\n",
    "    df_met_plot = df_met_plot.set_index('Date Time')\n",
    "\n",
    "    # Define Seasons\n",
    "    def get_season_local(month):\n",
    "        if month in [12, 1, 2]: return 'Winter'\n",
    "        elif month in [3, 4, 5]: return 'Spring'\n",
    "        elif month in [6, 7, 8]: return 'Summer'\n",
    "        else: return 'Autumn'\n",
    "\n",
    "    df_met_plot['Season'] = df_met_plot.index.month.map(get_season_local)\n",
    "    season_order = ['Autumn', 'Winter', 'Spring', 'Summer']\n",
    "\n",
    "    # Setup the 1x3 Plotting Grid\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define the parameters to plot (Column Name, Label, Color)\n",
    "    plot_params = [\n",
    "        ('Temp', 'Temperature (F)', 'tab:red'),\n",
    "        ('NO2',  'NO2 (ppb)',         'tab:brown'),\n",
    "        ('O3',   'O3 (ppb)',          'tab:blue')\n",
    "    ]\n",
    "\n",
    "    # Generate Boxplots\n",
    "    for i, (col, label, color) in enumerate(plot_params):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Check if column exists before plotting\n",
    "        if col in df_met_plot.columns:\n",
    "            sns.boxplot(\n",
    "                data=df_met_plot,\n",
    "                x='Season',\n",
    "                y=col,\n",
    "                order=season_order,\n",
    "                ax=ax,\n",
    "                color=color,\n",
    "                showfliers=False\n",
    "            )\n",
    "            ax.set_ylabel(f'{label} ')\n",
    "            ax.set_xlabel('')\n",
    "            ax.tick_params(axis='x', labelsize=12)\n",
    "\n",
    "            # --- ADD CELSIUS AXIS ---\n",
    "            if col == 'Temp':\n",
    "                # Define conversion functions\n",
    "                def f_to_c(x): return (x - 32) * 5 / 9\n",
    "                def c_to_f(x): return (x * 9 / 5) + 32\n",
    "\n",
    "                # Create secondary axis on the right\n",
    "                secax = ax.secondary_yaxis('right', functions=(f_to_c, c_to_f))\n",
    "                secax.set_ylabel('Temperature (C)')\n",
    "            # ----------------------------------\n",
    "\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'Column {col} not found', ha='center', va='center')\n",
    "\n",
    "    plt.suptitle('Seasonal Trends: Temp, NO2, and O3', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 4. Save the Figure\n",
    "    # Make sure 'output_folder' is defined, otherwise replace with a string path like r'C:\\Users\\...'\n",
    "    if 'output_folder' in locals():\n",
    "        output_filename = os.path.join(output_folder, 'Seasonal_Boxplots_Met_Gases.png')\n",
    "        plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {output_filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# DATA COVERAGE CHECK: Months per Season\n",
    "# =========================================================\n",
    "\n",
    "# Create a temporary dataframe for calculation\n",
    "df_counts = df_met_data.copy()\n",
    "df_counts['Date Time'] = pd.to_datetime(df_counts['Date Time'])\n",
    "df_counts = df_counts.set_index('Date Time')\n",
    "\n",
    "# Helper for coverage check\n",
    "def get_season_check(month):\n",
    "    if month in [12, 1, 2]: return 'Winter'\n",
    "    elif month in [3, 4, 5]: return 'Spring'\n",
    "    elif month in [6, 7, 8]: return 'Summer'\n",
    "    else: return 'Autumn'\n",
    "\n",
    "df_counts['Season'] = df_counts.index.month.map(get_season_check)\n",
    "\n",
    "# Count unique Year-Month periods\n",
    "months_per_season = df_counts.groupby('Season').apply(lambda x: x.index.to_period('M').nunique())\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" DATA COVERAGE (Months per Season)\")\n",
    "print(\"=\"*30)\n",
    "season_order = ['Autumn', 'Winter', 'Spring', 'Summer']\n",
    "\n",
    "for season in season_order:\n",
    "    count = months_per_season.get(season, 0)\n",
    "    print(f\"{season:<10}: {count} months\")\n",
    "print(\"=\"*30 + \"\\n\")"
   ],
   "id": "3606f7fbf3f2d1d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seasonal Diurnals SMPS data",
   "id": "7a2ac2afd6b4a41f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare Data\n",
    "# ---------------------------------------------------------\n",
    "# Ensure df_hourly exists\n",
    "df_diurnal = df_hourly.copy()\n",
    "\n",
    "# Ensure index is datetime\n",
    "if not isinstance(df_diurnal.index, pd.DatetimeIndex):\n",
    "    df_diurnal.index = pd.to_datetime(df_diurnal.index)\n",
    "\n",
    "# Map Months to Seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "df_diurnal['Season'] = df_diurnal.index.month.map(get_season)\n",
    "df_diurnal['Hour'] = df_diurnal.index.hour\n",
    "\n",
    "# Calculate Mean Hourly Values per Season\n",
    "seasonal_means = df_diurnal.groupby(['Season', 'Hour'])[['PM0.1', 'PM1']].mean().reset_index()\n",
    "\n",
    "# Create 4-Panel Plot\n",
    "# ---------------------------------------------------------\n",
    "seasons = ['Autumn', 'Winter', 'Spring', 'Summer']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10), sharex=True)\n",
    "axes = axes.flatten()  # Flatten 2D array of axes for easy looping\n",
    "\n",
    "for i, season in enumerate(seasons):\n",
    "    ax1 = axes[i]\n",
    "\n",
    "    # Filter data for the current season\n",
    "    data = seasonal_means[seasonal_means['Season'] == season]\n",
    "\n",
    "    # --- Left Axis: PM0.1 (Blue) ---\n",
    "    color1 = 'tab:blue'\n",
    "    ax1.set_xlabel('Hour of Day')\n",
    "    ax1.set_ylabel('PM0.1 (µg/m³)', color=color1, fontsize=12)\n",
    "    line1 = ax1.plot(data['Hour'], data['PM0.1'], color=color1, linewidth=2, marker='o', markersize=4, label='PM0.1')\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "    # --- Right Axis: PM1 (Red) ---\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = 'tab:red'\n",
    "    ax2.set_ylabel('PM1 (µg/m³)', color=color2, fontsize=12)\n",
    "    line2 = ax2.plot(data['Hour'], data['PM1'], color=color2, linewidth=2, linestyle='--', marker='s', markersize=4, label='PM1')\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "    # Formatting\n",
    "    ax1.set_title(f'{season}', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(range(0, 24, 4)) # Ticks every 4 hours\n",
    "    ax1.set_xlim(0, 23)\n",
    "\n",
    "    # Optional: Align y-axis starts to 0 if desired\n",
    "    # ax1.set_ylim(bottom=0)\n",
    "    # ax2.set_ylim(bottom=0)\n",
    "\n",
    "    # Combine legends for this subplot\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper left', fontsize=10, frameon=True)\n",
    "\n",
    "plt.suptitle('Seasonal Diurnal Cycles: PM0.1 vs PM1', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Plot\n",
    "output_filename = os.path.join(output_folder, 'Seasonal_Diurnal_PM01_PM1_4Panel.png')\n",
    "plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to: {output_filename}\")"
   ],
   "id": "b8e354e32d3e224b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seasonal Diurnal Plots AQMD data (Temp, NO2, O3)",
   "id": "8e97fefbd962a7f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare Data\n",
    "# Create a working copy to avoid modifying the original dataframe\n",
    "df_diurnal = df_met_data.copy()\n",
    "\n",
    "# Ensure 'Date Time' is in datetime format\n",
    "if not pd.api.types.is_datetime64_any_dtype(df_diurnal['Date Time']):\n",
    "    df_diurnal['Date Time'] = pd.to_datetime(df_diurnal['Date Time'])\n",
    "\n",
    "# Extract Hour and Season\n",
    "df_diurnal['Hour'] = df_diurnal['Date Time'].dt.hour\n",
    "df_diurnal['Month'] = df_diurnal['Date Time'].dt.month\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]: return 'Winter'\n",
    "    elif month in [3, 4, 5]: return 'Spring'\n",
    "    elif month in [6, 7, 8]: return 'Summer'\n",
    "    else: return 'Autumn'\n",
    "\n",
    "df_diurnal['Season'] = df_diurnal['Month'].apply(get_season)\n",
    "\n",
    "# Setup Plotting\n",
    "# Parameters to plot: (Column Name, Label)\n",
    "params_to_plot = [\n",
    "    ('Temp', 'Temperature (F)'),\n",
    "    ('NO2',  'NO2 (ppb)'),\n",
    "    ('O3',   'O3 (ppb)')\n",
    "]\n",
    "\n",
    "season_order = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "# Define colors for consistency\n",
    "season_colors = {'Winter': 'tab:blue', 'Spring': 'tab:green', 'Summer': 'tab:red', 'Autumn': 'tab:orange'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "\n",
    "# Generate Plots\n",
    "for i, (col, label) in enumerate(params_to_plot):\n",
    "    ax = axes[i]\n",
    "\n",
    "    if col in df_diurnal.columns:\n",
    "        # sns.lineplot automatically calculates the mean and confidence intervals (shaded area)\n",
    "        sns.lineplot(\n",
    "            data=df_diurnal,\n",
    "            x='Hour',\n",
    "            y=col,\n",
    "            hue='Season',\n",
    "            hue_order=season_order,\n",
    "            palette=season_colors,\n",
    "            ax=ax,\n",
    "            ci=95\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel('Hour of Day (LT)', fontsize=12)\n",
    "        ax.set_ylabel(label, fontsize=16)\n",
    "        ax.set_xticks(range(0, 24, 4)) # Ticks every 4 hours\n",
    "\n",
    "        # --- ADD CELSIUS AXIS ---\n",
    "        if col == 'Temp':\n",
    "            # Define conversion functions\n",
    "            def f_to_c(x): return (x - 32) * 5 / 9\n",
    "            def c_to_f(x): return (x * 9 / 5) + 32\n",
    "\n",
    "            # Create secondary axis on the right\n",
    "            secax = ax.secondary_yaxis('right', functions=(f_to_c, c_to_f))\n",
    "            secax.set_ylabel('Temperature (C)', fontsize=16)\n",
    "        # ----------------------------------\n",
    "\n",
    "        # Only show legend on the last plot to save space, or adjust as needed\n",
    "        if i == 2:\n",
    "            ax.legend(title='Season', loc='best')\n",
    "        else:\n",
    "            # Check if legend exists before removing (some versions of seaborn behave differently)\n",
    "            if ax.get_legend() is not None:\n",
    "                ax.get_legend().remove()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{col} not found', ha='center', va='center')\n",
    "\n",
    "plt.suptitle('Seasonal Diurnal Cycles', fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save"
   ],
   "id": "1899e0aa57d6c4f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Merge Particle Data (df_hourly) with Met Data (df_met_data)\n",
    "# ---------------------------------------------------------\n",
    "# Check if dataframes exist\n",
    "if 'df_hourly' not in locals():\n",
    "    print(\"Error: 'df_hourly' not found. Please run the 'PM0.1 and PM1 calculations' cell first.\")\n",
    "elif 'df_met_data' not in locals():\n",
    "    print(\"Error: 'df_met_data' not found. Please run the meteorological data import cell first.\")\n",
    "else:\n",
    "    # Prepare df_hourly (Index is usually already datetime, but ensure it)\n",
    "    df_pm = df_hourly.copy()\n",
    "    if not isinstance(df_pm.index, pd.DatetimeIndex):\n",
    "        df_pm.index = pd.to_datetime(df_pm.index)\n",
    "\n",
    "    # Prepare df_met_data (Set 'Date Time' as index for merging)\n",
    "    df_met = df_met_data.copy()\n",
    "    if 'Date Time' in df_met.columns:\n",
    "        df_met['Date Time'] = pd.to_datetime(df_met['Date Time'])\n",
    "        df_met = df_met.set_index('Date Time')\n",
    "\n",
    "    # Merge on index (nearest hour match or exact match)\n",
    "    # Using inner join to keep only timestamps present in both\n",
    "    df_merged = pd.merge(df_pm, df_met, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    # 2. Define Pairs to Correlate\n",
    "    # ---------------------------------------------------------\n",
    "    # Format: (PM_Column, Met_Column, Display_Title)\n",
    "    # Note: Ensure these column names match your data exactly\n",
    "    pairs = [\n",
    "        ('PM0.1', 'Temp', 'PM0.1 vs Temperature'),\n",
    "        ('PM1',   'Temp', 'PM1 vs Temperature'),\n",
    "        ('PM0.1', 'NO2',  'PM0.1 vs NO2'),\n",
    "        ('PM1',   'NO2',  'PM1 vs NO2')\n",
    "    ]\n",
    "\n",
    "    # 3. Calculate Daily R2\n",
    "    # ---------------------------------------------------------\n",
    "    daily_r2_data = []\n",
    "\n",
    "    # Group by Date\n",
    "    for date, day_group in df_merged.groupby(df_merged.index.date):\n",
    "        # Determine Season for this date\n",
    "        month = pd.to_datetime(date).month\n",
    "        if month in [12, 1, 2]: season = 'Winter'\n",
    "        elif month in [3, 4, 5]: season = 'Spring'\n",
    "        elif month in [6, 7, 8]: season = 'Summer'\n",
    "        else: season = 'Autumn'\n",
    "\n",
    "        # Dictionary to store results for this day\n",
    "        day_result = {'Season': season, 'Date': date}\n",
    "\n",
    "        # Calculate R2 for each pair\n",
    "        for pm_col, met_col, label in pairs:\n",
    "            # Drop NaNs for this specific pair\n",
    "            valid_data = day_group[[pm_col, met_col]].dropna()\n",
    "\n",
    "            # Require at least 12 hours of data to calculate a valid correlation\n",
    "            if len(valid_data) >= 12:\n",
    "                corr = valid_data[pm_col].corr(valid_data[met_col])\n",
    "                day_result[label] = corr ** 2 # R-squared\n",
    "            else:\n",
    "                day_result[label] = np.nan\n",
    "\n",
    "        daily_r2_data.append(day_result)\n",
    "\n",
    "    df_r2 = pd.DataFrame(daily_r2_data)\n",
    "\n",
    "    # 4. Generate Plot\n",
    "    # ---------------------------------------------------------\n",
    "    season_order = ['Autumn','Winter', 'Spring', 'Summer', ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (pm_col, met_col, label) in enumerate(pairs):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Plot if data exists\n",
    "        if label in df_r2.columns:\n",
    "            sns.boxplot(\n",
    "                data=df_r2,\n",
    "                x='Season',\n",
    "                y=label,\n",
    "                order=season_order,\n",
    "                ax=ax,\n",
    "                color='skyblue',\n",
    "                showfliers=False # Optional: Hide outliers for cleaner view\n",
    "            )\n",
    "            ax.set_title(label, fontsize=14)\n",
    "            ax.set_ylabel('Daily $R^2$', fontsize=12)\n",
    "            ax.set_xlabel('')\n",
    "            ax.grid(True, linestyle=':', alpha=0.6)\n",
    "            ax.set_ylim(0, 1.05) # R2 is always 0-1\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'Data for {label} missing', ha='center')\n",
    "\n",
    "    plt.suptitle('Seasonal Trends: Daily Correlations ($R^2$) of Hourly Averages', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    output_filename = os.path.join(output_folder, 'Seasonal_R2_Boxplots.png')\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to: {output_filename}\")"
   ],
   "id": "3a0ad1292324de08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seasonal Weekday Diurnals PM0.1 vs PM1",
   "id": "49309cf6ae78f677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Prepare Data\n",
    "# ---------------------------------------------------------\n",
    "# Ensure df_hourly exists (from Cell 26)\n",
    "if 'df_hourly' not in locals():\n",
    "    raise ValueError(\"variable 'df_hourly' not found. Please run the 'PM0.1 and PM1 calculations' cell first.\")\n",
    "\n",
    "df_weekly = df_hourly.copy()\n",
    "\n",
    "# Ensure index is datetime\n",
    "if not isinstance(df_weekly.index, pd.DatetimeIndex):\n",
    "    df_weekly.index = pd.to_datetime(df_weekly.index)\n",
    "\n",
    "# Map Months to Seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "df_weekly['Season'] = df_weekly.index.month.map(get_season)\n",
    "df_weekly['DayOfWeek'] = df_weekly.index.dayofweek # Mon=0, Sun=6\n",
    "df_weekly['Hour'] = df_weekly.index.hour\n",
    "\n",
    "# Group by Season, DayOfWeek, and Hour to get the average weekly cycle\n",
    "# This creates a composite week for each season\n",
    "weekly_means = df_weekly.groupby(['Season', 'DayOfWeek', 'Hour'])[['PM0.1', 'PM1']].mean()\n",
    "\n",
    "# 2. Create 4-Panel Plot\n",
    "# ---------------------------------------------------------\n",
    "seasons = ['Autumn', 'Winter', 'Spring', 'Summer']\n",
    "# Days labels for x-axis\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, season in enumerate(seasons):\n",
    "    ax1 = axes[i]\n",
    "\n",
    "    # Extract data for the season\n",
    "    if season in weekly_means.index.get_level_values('Season'):\n",
    "        data = weekly_means.loc[season]\n",
    "\n",
    "        # Create a continuous hourly index for the week (0 to 167)\n",
    "        # Data is already sorted by DayOfWeek then Hour due to groupby\n",
    "        # We assume 24 hours for each of the 7 days\n",
    "\n",
    "        # Flatten the data for plotting\n",
    "        pm01_vals = data['PM0.1'].values\n",
    "        pm1_vals = data['PM1'].values\n",
    "\n",
    "        # Create x-axis (0 to 167 hours)\n",
    "        x_hours = np.arange(len(pm01_vals))\n",
    "\n",
    "        # --- Left Axis: PM0.1 (Blue) ---\n",
    "        color1 = 'tab:blue'\n",
    "        line1 = ax1.plot(x_hours, pm01_vals, color=color1, linewidth=1.5, label='PM0.1')\n",
    "        ax1.set_ylabel('PM0.1 (µg/m³)', color=color1, fontsize=12)\n",
    "        ax1.tick_params(axis='y', labelcolor=color1)\n",
    "        ax1.grid(True, linestyle=':', alpha=0.6, axis='x') # Grid lines for days\n",
    "\n",
    "        # --- Right Axis: PM1 (Red) ---\n",
    "        ax2 = ax1.twinx()\n",
    "        color2 = 'tab:red'\n",
    "        line2 = ax2.plot(x_hours, pm1_vals, color=color2, linewidth=1.5, linestyle='--', label='PM1')\n",
    "        ax2.set_ylabel('PM1 (µg/m³)', color=color2, fontsize=12)\n",
    "        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "        # Formatting\n",
    "        ax1.set_title(f'{season}', fontsize=14, fontweight='bold')\n",
    "\n",
    "        # Set x-ticks to be the start of each day (every 24 hours)\n",
    "        # Center labels slightly if desired, or just put them at 00:00\n",
    "        ax1.set_xticks(np.arange(0, 168, 24))\n",
    "        ax1.set_xticklabels(days)\n",
    "        ax1.set_xlim(0, 167)\n",
    "\n",
    "        # Add vertical lines to separate days\n",
    "        for x in range(24, 168, 24):\n",
    "            ax1.axvline(x=x, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "        # Legend\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax1.legend(lines, labels, loc='upper left', fontsize=10, frameon=True)\n",
    "\n",
    "# Add shared x-axis label\n",
    "fig.text(0.5, 0.04, 'Day of Week', ha='center', fontsize=14)\n",
    "\n",
    "plt.suptitle('Weekly Diurnal Cycles (Monday-Sunday): PM0.1 vs PM1', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Plot\n",
    "output_filename = os.path.join(output_folder, 'Seasonal_Weekly_Cycle_PM01_PM1.png')\n",
    "plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Plot saved to: {output_filename}\")"
   ],
   "id": "affbf1fb8807e187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seasonal diurnal coloplot dN/dlogDp",
   "id": "9042f9ca25a8f949"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- USER CONFIGURATION ---\n",
    "try:\n",
    "    # 1. Get Particle Data Matrix (High Resolution)\n",
    "    if 'dNdlogDp_stpp' in locals():\n",
    "        data_matrix = dNdlogDp_stpp\n",
    "    else:\n",
    "        # Fallback if variable names are different\n",
    "        data_matrix = dNdlogDp\n",
    "\n",
    "    # 2. Get Diameter Bins\n",
    "    if 'mid_Dp' in locals():\n",
    "        diameter_bins = mid_Dp\n",
    "    else:\n",
    "        diameter_bins = mid_points\n",
    "\n",
    "    # 3. Get Time Index (MUST MATCH data_matrix length)\n",
    "    # We use tsdfp.index (original high-res index) instead of df_hourly.index\n",
    "    if 'tsdfp' in locals():\n",
    "        time_index = tsdfp.index\n",
    "    else:\n",
    "        # If tsdfp is missing, try to reconstruct from dfp\n",
    "        time_index = pd.to_datetime(dfp['local_time'])\n",
    "\n",
    "    # Verification Print\n",
    "    print(f\"Data Shape: {data_matrix.shape}\")\n",
    "    print(f\"Time Index Length: {len(time_index)}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Variable missing: {e}\")\n",
    "    print(\"Please ensure you have run the cells that define 'dNdlogDp_stpp' and 'tsdfp'.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Prepare Data\n",
    "# ---------------------------------------------------------\n",
    "# Create DataFrame using the HIGH RESOLUTION index\n",
    "df_dist = pd.DataFrame(data_matrix, index=time_index, columns=diameter_bins)\n",
    "\n",
    "# Ensure data types are correct (fixes the \"log10\" error)\n",
    "df_dist.columns = df_dist.columns.astype(float)\n",
    "df_dist = df_dist.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Handle zeros for Log Plot\n",
    "df_dist = df_dist.replace(0, np.nan)\n",
    "\n",
    "# Define Season Function\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "# Add grouping columns\n",
    "# We convert index to datetime just in case it isn't already\n",
    "df_dist.index = pd.to_datetime(df_dist.index)\n",
    "df_dist['Season'] = df_dist.index.month.map(get_season)\n",
    "df_dist['Hour'] = df_dist.index.hour\n",
    "\n",
    "# 2. Group by Season and Hour\n",
    "# ---------------------------------------------------------\n",
    "print(\"Calculating seasonal hourly averages...\")\n",
    "# This might take a few seconds due to the large dataset size\n",
    "seasonal_diurnal = df_dist.groupby(['Season', 'Hour']).mean()\n",
    "\n",
    "# 3. Generate Plots\n",
    "# ---------------------------------------------------------\n",
    "season_order = ['Autumn', 'Winter', 'Spring', 'Summer']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Color Scale Settings (Adjust vmin/vmax if plot is too dark/light)\n",
    "cmap = plt.cm.jet\n",
    "vmin = 1e2\n",
    "vmax = 1e5\n",
    "\n",
    "for i, season in enumerate(season_order):\n",
    "    ax = axes[i]\n",
    "\n",
    "    if season in seasonal_diurnal.index.get_level_values(0):\n",
    "        data_season = seasonal_diurnal.loc[season]\n",
    "\n",
    "        # Prepare grids\n",
    "        X = data_season.index.astype(float)   # Hours (0-23)\n",
    "        Y = data_season.columns.astype(float) # Diameters\n",
    "        Z = data_season.values.T              # Concentration\n",
    "\n",
    "        # Plot\n",
    "        mesh = ax.pcolormesh(X, Y, Z, cmap=cmap, norm=LogNorm(vmin=vmin, vmax=vmax), shading='auto')\n",
    "\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title(f'{season}', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Labels\n",
    "        if i in [0, 2]:\n",
    "            ax.set_ylabel('Particle Diameter (nm)', fontsize=14)\n",
    "        if i in [2, 3]:\n",
    "            ax.set_xlabel('Hour of Day', fontsize=14)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xticks(np.arange(0, 25, 4))\n",
    "        ax.set_yticks([10, 20, 50, 100, 200, 500])\n",
    "        ax.get_yaxis().set_major_formatter(ScalarFormatter())\n",
    "        ax.grid(True, which='both', linestyle=':', alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No Data', ha='center', transform=ax.transAxes)\n",
    "\n",
    "# 4. Colorbar and Save\n",
    "# ---------------------------------------------------------\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(mesh, cax=cbar_ax)\n",
    "cbar.set_label('dN/dlogDp (#/cm³)', fontsize=14)\n",
    "\n",
    "plt.suptitle('Seasonal Diurnals of Particle Size Distribution', fontsize=20)\n",
    "plt.subplots_adjust(right=0.9, wspace=0.1, hspace=0.15)\n",
    "\n",
    "if 'output_folder' not in locals(): output_folder = '.'\n",
    "plot_filename = os.path.join(output_folder, 'Seasonal_Diurnals_Colormap.png')\n",
    "plt.savefig(plot_filename, dpi=300)\n",
    "plt.show()\n",
    "print(f\"Plot saved to: {plot_filename}\")"
   ],
   "id": "99a81455e9f147f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seasonal diurnal coloplot dM/dlogDp",
   "id": "1b62d1a90c2a0d10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- USER CONFIGURATION ---\n",
    "try:\n",
    "    # 1. Get Mass Data Matrix (dM/dlogDp)\n",
    "    # Check for the variable calculated in 'PM0.1 and PM1 calculations' cell\n",
    "    if 'dMdlogDp_stpp' in locals():\n",
    "        data_matrix = dMdlogDp_stpp\n",
    "    else:\n",
    "        # Fallback: Try to calculate it if missing (assuming spherical particles, density=1.2)\n",
    "        # Note: This is an approximation if the original variable is lost\n",
    "        print(\"Variable 'dMdlogDp_stpp' not found. Attempting to calculate from 'dNdlogDp_stpp'...\")\n",
    "        if 'dNdlogDp_stpp' in locals() and 'mid_Dp' in locals():\n",
    "             # density = 1.2 g/cm3 (standard assumption if unknown)\n",
    "             # Formula: dM = dN * (pi/6) * Dp^3 * density * unit_conversion\n",
    "             # We assume mid_Dp is in nm. Conversion factor handles nm->cm and g->ug->m3\n",
    "             density = 1.0\n",
    "             data_matrix = dNdlogDp_stpp * (np.pi/6) * (mid_Dp**3) * density * 1e-9\n",
    "        else:\n",
    "             raise NameError(\"dMdlogDp_stpp\")\n",
    "\n",
    "    # 2. Get Diameter Bins\n",
    "    if 'mid_Dp' in locals():\n",
    "        diameter_bins = mid_Dp\n",
    "    else:\n",
    "        diameter_bins = mid_points\n",
    "\n",
    "    # 3. Get Time Index\n",
    "    if 'tsdfp' in locals():\n",
    "        time_index = tsdfp.index\n",
    "    else:\n",
    "        time_index = pd.to_datetime(dfp['local_time'])\n",
    "\n",
    "    print(f\"Data Shape: {data_matrix.shape}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"⚠️ Variable missing: {e}\")\n",
    "    print(\"Please run the 'PM0.1 and PM1 calculations' cell first to generate 'dMdlogDp_stpp'.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Prepare Data\n",
    "# ---------------------------------------------------------\n",
    "# Create DataFrame\n",
    "df_dist = pd.DataFrame(data_matrix, index=time_index, columns=diameter_bins)\n",
    "\n",
    "# Ensure data types are correct\n",
    "df_dist.columns = df_dist.columns.astype(float)\n",
    "df_dist = df_dist.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Handle zeros for Log Plot (Replace 0 with NaN so they appear white/transparent)\n",
    "df_dist = df_dist.replace(0, np.nan)\n",
    "\n",
    "# Define Season Function\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "# Add grouping columns\n",
    "df_dist.index = pd.to_datetime(df_dist.index)\n",
    "df_dist['Season'] = df_dist.index.month.map(get_season)\n",
    "df_dist['Hour'] = df_dist.index.hour\n",
    "\n",
    "# 2. Group by Season and Hour (Calculate Mean)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Calculating seasonal hourly averages for Mass...\")\n",
    "seasonal_diurnal = df_dist.groupby(['Season', 'Hour']).mean()\n",
    "\n",
    "# 3. Generate Plots\n",
    "# ---------------------------------------------------------\n",
    "season_order = ['Autumn', 'Winter', 'Spring', 'Summer']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Color Scale Settings for MASS\n",
    "# Mass concentrations are lower than Number concentrations.\n",
    "# Range typically 0.1 to 50 ug/m3 for these bins.\n",
    "cmap = plt.cm.jet\n",
    "vmin = 1e-2  # Lower limit (ug/m3)\n",
    "vmax = 3e2   # Upper limit (ug/m3)\n",
    "\n",
    "for i, season in enumerate(season_order):\n",
    "    ax = axes[i]\n",
    "\n",
    "    if season in seasonal_diurnal.index.get_level_values(0):\n",
    "        data_season = seasonal_diurnal.loc[season]\n",
    "\n",
    "        # Prepare grids\n",
    "        X = data_season.index.astype(float)   # Hours (0-23)\n",
    "        Y = data_season.columns.astype(float) # Diameters\n",
    "        Z = data_season.values.T              # Mass Concentration\n",
    "\n",
    "        # Plot\n",
    "        mesh = ax.pcolormesh(X, Y, Z, cmap=cmap, norm=LogNorm(vmin=vmin, vmax=vmax), shading='auto')\n",
    "\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title(f'{season}', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Labels\n",
    "        if i in [0, 2]:\n",
    "            ax.set_ylabel('Particle Diameter (nm)', fontsize=14)\n",
    "        if i in [2, 3]:\n",
    "            ax.set_xlabel('Hour of Day', fontsize=14)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xticks(np.arange(0, 25, 4))\n",
    "        ax.set_yticks([10, 20, 50, 100, 200, 500])\n",
    "        ax.get_yaxis().set_major_formatter(ScalarFormatter())\n",
    "        ax.grid(True, which='both', linestyle=':', alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No Data', ha='center', transform=ax.transAxes)\n",
    "\n",
    "# 4. Colorbar and Save\n",
    "# ---------------------------------------------------------\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(mesh, cax=cbar_ax)\n",
    "cbar.set_label('dM/dlogDp (µg/m³)', fontsize=14)\n",
    "\n",
    "plt.suptitle('Seasonal Diurnals of Mass Size Distribution', fontsize=20)\n",
    "plt.subplots_adjust(right=0.9, wspace=0.1, hspace=0.15)\n",
    "\n",
    "if 'output_folder' not in locals(): output_folder = '.'\n",
    "plot_filename = os.path.join(output_folder, 'Seasonal_Diurnals_Colormap_dM.png')\n",
    "plt.savefig(plot_filename, dpi=300)\n",
    "plt.show()\n",
    "print(f\"Mass Distribution plot saved to: {plot_filename}\")"
   ],
   "id": "19721576ec29c3b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from windrose import WindroseAxes\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# --- 1. PREPARE & MERGE DATA ---\n",
    "\n",
    "# Ensure df_hourly index is datetime (This contains PM0.1, PM1)\n",
    "if not isinstance(df_hourly.index, pd.DatetimeIndex):\n",
    "    df_hourly.index = pd.to_datetime(df_hourly.index)\n",
    "\n",
    "# Ensure df_met_data 'Date Time' is datetime and set as index (This contains WS, WD)\n",
    "# df_met_data comes from the earlier cells in your notebook\n",
    "if 'Date Time' in df_met_data.columns:\n",
    "    df_met_data = df_met_data.set_index('Date Time')\n",
    "df_met_data.index = pd.to_datetime(df_met_data.index)\n",
    "\n",
    "# Merge the two dataframes on their timestamps\n",
    "# 'inner' join ensures we only plot times where we have BOTH PM and Wind data\n",
    "df_rose = pd.merge(df_hourly, df_met_data, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# --- 2. DEFINE PLOTTING FUNCTION ---\n",
    "\n",
    "def plot_pollution_rose(data, pollutant_col, ws_col='WS', wd_col='WD'):\n",
    "    \"\"\"\n",
    "    Plots a pollution rose where:\n",
    "    - Direction (WD) = Wind Direction\n",
    "    - Length of bar = Frequency of wind from that direction\n",
    "    - Color = Concentration of the Pollutant (PM0.1 or PM1)\n",
    "    \"\"\"\n",
    "    # Remove missing values for the specific columns we are using\n",
    "    data_clean = data[[pollutant_col, ws_col, wd_col]].dropna()\n",
    "\n",
    "    # Initialize the Windrose Axis\n",
    "    ax = WindroseAxes.from_ax()\n",
    "\n",
    "    # Plot using bar style\n",
    "    # 'var' determines the values used for coloring (The Pollutant)\n",
    "    # 'direction' determines the angle (Wind Direction)\n",
    "    ax.bar(data_clean[wd_col], data_clean[pollutant_col],\n",
    "           normed=True,       # Normalize to show percentage frequencies\n",
    "           opening=0.8,       # Width of the bars (0.8 is standard)\n",
    "           edgecolor='white',\n",
    "           cmap=cm.jet,       # Colormap (Jet goes Blue->Red)\n",
    "           bins=[0, 1, 2, 5, 10, 15, 20]) # Adjust these bins based on your PM concentration levels!\n",
    "\n",
    "    # Add title and legend\n",
    "    ax.set_title(f\"Pollution Rose: {pollutant_col} (Color = Conc.)\")\n",
    "    ax.set_legend(title=f\"{pollutant_col} ($ \\mu g/m^3 $)\", loc='best', bbox_to_anchor=(1.1, 1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. GENERATE THE MAPS ---\n",
    "\n",
    "print(\"Pollution Rose for PM0.1\")\n",
    "plot_pollution_rose(df_rose, 'PM0.1')\n",
    "\n",
    "print(\"Pollution Rose for PM1\")\n",
    "plot_pollution_rose(df_rose, 'PM1')"
   ],
   "id": "26fe1a9f1f3957de",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a79ad63c-a695-4e58-800f-30c179424bc8",
   "metadata": {},
   "source": [
    "# Add Black Carbon Data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dNdlogDp_stpp.shape",
   "id": "ad687ea00e4aaab2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add N_stp array to the corresponding DataFrame\n",
    "dfp['N_stp'] = N_stp\n"
   ],
   "id": "28bd0ac72afffe18",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80a7e333-c7c6-4efb-883b-e957d4fdfc6c",
   "metadata": {},
   "source": [
    "## Concatenate all csv files in a folder for Aethelometer data"
   ]
  },
  {
   "cell_type": "code",
   "id": "df78276f-7787-409d-ad82-bbf5c46aa7b1",
   "metadata": {},
   "source": [
    "# concatenate\n",
    "df_list = (pd.read_csv(file) for file in aeth_csv_files)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "dfb   = pd.concat(df_list, ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "dfb['time'] = pd.to_datetime(dfb['time'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a27dd69d-1ab1-4ecb-b3ce-539f8c45328d",
   "metadata": {},
   "source": [
    "## Convert to local time"
   ]
  },
  {
   "cell_type": "code",
   "id": "e75cb83f-b8ad-4b95-b54e-b2d9d3acc03d",
   "metadata": {},
   "source": [
    "def convert_to_local(df):\n",
    "\n",
    "    dfb_local = df.copy()\n",
    "\n",
    "    # Local timezone (replace with your local timezone)\n",
    "    source_timezone = pytz.timezone('UTC')\n",
    "    local_timezone = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "    # Use the tz_convert function to convert the datetime column\n",
    "    dfb_local['time'] = df['time'].dt.tz_localize(source_timezone).dt.tz_convert(local_timezone)\n",
    "    dfb_local['time'] = pd.to_datetime(dfb_local[\"time\"].dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    dfb_local.rename(columns={'time':'local_time_bc'}, inplace=True)\n",
    "    return dfb_local"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0280e77d-2875-4ef3-be1a-3293420a79eb",
   "metadata": {},
   "source": [
    "## Call the local time conversion function, convert_to_local, to get the new df, dfb_local, which has the aethelometer data now in local time"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d2ed16c-2176-465f-b41d-a34587b37b3b",
   "metadata": {},
   "source": [
    "dfb_local = convert_to_local(dfb)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4e80731-90c9-4850-b5c4-0b74c6e804df",
   "metadata": {},
   "source": [
    "Calculate the rolling 30 minute average of the vehicular black carbon concentration (EBC_6 in aethelometer data)\n",
    "\n",
    "Make a merged dataframe which contains both SMPS and Aethelometer data across consistent timestamps\n",
    "\n",
    "Add a column to the original SMPS dataframe which represents the rolling 30 minute average black carbon, here titled 30_min_avg (from the column EBC_6 in aethelometer data) "
   ]
  },
  {
   "cell_type": "code",
   "id": "19e93d5f-d54f-456a-8b02-cd04974ad161",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# --- Step 1: Ensure datetime and set index for rolling calculation ---\n",
    "dfb_local['local_time_bc'] = pd.to_datetime(dfb_local['local_time_bc'])\n",
    "dfb_local = dfb_local.sort_values('local_time_bc').set_index('local_time_bc')\n",
    "\n",
    "# --- Step 2: Calculate rolling 30-min average for EBC_6 ---\n",
    "dfb_local['30_min_avg'] = dfb_local['EBC_6'].rolling('30T').mean()\n",
    "\n",
    "# --- Step 3: Reset index so local_time_bc is a column again ---\n",
    "dfb_local = dfb_local.reset_index()\n",
    "\n",
    "# --- Step 4: Ensure dfp datetime is correct ---\n",
    "dfp['local_time'] = pd.to_datetime(dfp['local_time'])\n",
    "\n",
    "# --- Step 5: Merge using merge_asof ---\n",
    "merged_df = pd.merge_asof(\n",
    "    dfp.sort_values('local_time'),\n",
    "    dfb_local[['local_time_bc', '30_min_avg']].sort_values('local_time_bc'),\n",
    "    left_on='local_time',\n",
    "    right_on='local_time_bc',\n",
    "    direction='nearest',\n",
    "    suffixes=('', '_bc')  # avoid naming conflicts\n",
    ")\n",
    "\n",
    "\n",
    "# --- Step 6: Add the rolling average column to dfp ---\n",
    "if '30_min_avg' in merged_df.columns:\n",
    "    dfp['30_min_avg'] = merged_df['30_min_avg']\n",
    "else:\n",
    "    raise KeyError(\"⚠ '30_min_avg' not found in merged_df. Check dfb_local generation.\")\n",
    "\n",
    "print(dfp[['local_time', '30_min_avg']].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86666970-178d-41f1-aa35-e8873698aa86",
   "metadata": {},
   "source": [
    "dfp # can check to ensure 30_min_avg column was added to dfp here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96eef8b8-3eb7-4d40-b318-2a78afe18c12",
   "metadata": {},
   "source": [
    "# Plotting Section "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ed2dd-36a1-4041-afdc-7a7737e4fa43",
   "metadata": {},
   "source": [
    "## Time series colormap plot for every day in the dataset (x=time, y=diameter (nm), z=dNdlogDp)\n",
    "\n",
    "Here, I use a warped colormap (a cubic function instead of linear) which emphasizes the variation in the high concentrations"
   ]
  },
  {
   "cell_type": "code",
   "id": "e554d654-f1d0-4688-8efe-70a56dd662f3",
   "metadata": {},
   "source": [
    "# --- Warped colormap setup ---\n",
    "def warp_colormap(cmap, warp_func, N=256):\n",
    "    orig = np.linspace(0, 1, N)\n",
    "    warped = warp_func(orig)\n",
    "    warped = np.clip(warped, 0, 1)\n",
    "    warped_colors = cmap(warped)\n",
    "    return LinearSegmentedColormap.from_list(\"warped_cmap\", warped_colors)\n",
    "\n",
    "def high_end_bias(x):\n",
    "    return x**3\n",
    "\n",
    "# Color list\n",
    "colors = ['#313695', '#4575b4', '#74add1', '#a6cfe2', '#fddcaa', \n",
    "          '#fdae61', '#f46d43', '#d73027', '#a50026']\n",
    "base_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "custom_cmap = warp_colormap(base_cmap, high_end_bias)\n",
    "\n",
    "# --- Data setup ---\n",
    "dfp['local_time'] = pd.to_datetime(dfp['local_time'])\n",
    "dfp['date'] = dfp['local_time'].dt.date\n",
    "\n",
    "# Meshgrid setup (once, reused)\n",
    "Time = pd.to_datetime(dfp['local_time'])\n",
    "X = mdates.date2num(Time)\n",
    "Y = mid_Dp.copy()\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "Z = dNdlogDp_stpp.copy().T\n",
    "Z_all = np.ma.masked_invalid(np.asarray(Z, dtype=np.float64))\n",
    "\n",
    "# --- Daily plot loop ---\n",
    "plot_counter = 0\n",
    "grouped = dfp.groupby('date')\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "for date, group in grouped:\n",
    "    day_mask = dfp['date'] == date\n",
    "    x_day = X[day_mask]\n",
    "    z_day = Z_all[:, day_mask]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 5))\n",
    "    pcm = ax1.pcolormesh(x_day, YY[:, :z_day.shape[1]], z_day, shading='auto',\n",
    "                         cmap=custom_cmap, norm=LogNorm(vmin=10, vmax=1e5)) # jt\n",
    "\n",
    "    cbar = fig.colorbar(pcm, ax=ax1)\n",
    "    cbar.set_label('$\\\\dfrac{dN}{dlogDp}$ (# cm$^{-3}$)')    \n",
    "\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylim(13, 800)\n",
    "\n",
    "    # Custom y-tick locations and labels\n",
    "    yticks = [13, 20, 30, 40, 50, 60, 80, 100, 200, 300, 400, 500, 600, 700, 800]\n",
    "    ax1.set_yticks(yticks)\n",
    "    label_ticks = {13: '13', 100: '100', 800: '800'}\n",
    "    ax1.yaxis.set_major_formatter(FuncFormatter(lambda y, _: label_ticks.get(y, '')))\n",
    "\n",
    "    ax1.set_ylabel('Dp (nm)')\n",
    "    ax1.set_xlabel('local_time')\n",
    "    ax1.tick_params(axis='both', which='major')\n",
    "\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%H'))\n",
    "    ax1.set_xlim(x_day[0], x_day[-1])\n",
    "    \n",
    "    ax1.set_title(f'test pico SMPS - {date}')    \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save only once here\n",
    "    filename = os.path.join(output_folder, f'pico_time_series_{date}.png')\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close to free memory, no need to call plt.show()\n",
    "\n",
    "    plot_counter += 1\n",
    "\n",
    "print(f'Number of days plotted: {plot_counter}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c88b8eeb-4f12-4498-8094-0ba2009d644d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4d9a6e8-86b4-4ddd-922b-f6eb7fce373d",
   "metadata": {},
   "source": [
    "# Plot with 4 time series in the top plots and their respective N_stps in the lower plots"
   ]
  },
  {
   "cell_type": "code",
   "id": "16905c7b-b96a-4381-a008-e36f6f09b0d7",
   "metadata": {},
   "source": [
    "# Move to other code, keep this one too \n",
    "# --- Warped colormap setup ---\n",
    "def warp_colormap(cmap, warp_func, N=256):\n",
    "    orig = np.linspace(0, 1, N)\n",
    "    warped = warp_func(orig)\n",
    "    warped = np.clip(warped, 0, 1)\n",
    "    warped_colors = cmap(warped)\n",
    "    return LinearSegmentedColormap.from_list(\"warped_cmap\", warped_colors)\n",
    "\n",
    "def high_end_bias(x):\n",
    "    return x**3\n",
    "\n",
    "\n",
    "# Color list\n",
    "colors = ['#313695', '#4575b4', '#74add1', '#a6cfe2', '#fddcaa', \n",
    "          '#fdae61', '#f46d43', '#d73027', '#a50026']\n",
    "base_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "custom_cmap = warp_colormap(base_cmap, high_end_bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_day_data(df, Z_all, mid_Dp, date_str):\n",
    "    date = pd.to_datetime(date_str).date()\n",
    "    df = df.copy()\n",
    "    df['local_time'] = pd.to_datetime(df['local_time'])\n",
    "    df['date'] = df['local_time'].dt.date\n",
    "\n",
    "    mask = df['date'] == date\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(f\"No data found for date {date_str} in the dataframe.\")\n",
    "\n",
    "    df_day = df[mask]\n",
    "    X = mdates.date2num(df_day['local_time'])\n",
    "\n",
    "    z_day = Z_all[:, mask.values]  # Use .values to index numpy array\n",
    "\n",
    "    return X, mid_Dp, z_day, df_day\n",
    "# Define which days and sites to show\n",
    "plot_info = [\n",
    "    (dfp, dNdlogDp_stpp.T, mid_Dp, '2024-05-04', 'Pico Rivera - Example 1', 1e5),\n",
    "    (dfp, dNdlogDp_stpp.T, mid_Dp, '2024-05-05', 'Pico Rivera - Example 2', 1e5),\n",
    "    (dfp, dNdlogDp_stpp.T, mid_Dp, '2024-05-06', 'Pico Rivera - Example 3', 1e5),\n",
    "    (dfp, dNdlogDp_stpp.T, mid_Dp, '2024-05-07', 'Pico Rivera - Example 4', 1e5),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = GridSpec(4, 2, height_ratios=[3, 2, 3, 2], hspace=0.75, wspace=0.3)\n",
    "\n",
    "for i, (df, Z, mid_D, date, name, vmax) in enumerate(plot_info):\n",
    "    Z_masked = np.ma.masked_invalid(np.asarray(Z, dtype=np.float64))\n",
    "    X, Y, z_day, df_day = get_day_data(df, Z_masked, mid_D, date)\n",
    "\n",
    "    col_row = 0 if i < 2 else 2\n",
    "    col_col = i % 2\n",
    "\n",
    "    start = pd.to_datetime(date + ' 00:00')\n",
    "    end = pd.to_datetime(date + ' 23:59')\n",
    "\n",
    "    # meshgrid for pcolormesh\n",
    "    XX, YY = np.meshgrid(X, Y)\n",
    "\n",
    "    # --- Colormap plot ---\n",
    "    ax1 = fig.add_subplot(gs[col_row, col_col])\n",
    "    pcm = ax1.pcolormesh(XX, YY, z_day, shading='auto',\n",
    "                         cmap=custom_cmap, norm=LogNorm(vmin=10, vmax=vmax))\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylim(13, 800)\n",
    "    ax1.set_xlim(start, end)\n",
    "    yticks = [13, 20, 30, 40, 50, 60, 80, 100, 200, 300, 400, 500, 600, 700, 800]\n",
    "    ax1.set_yticks(yticks)\n",
    "    label_ticks = {13: '13', 100: '100', 800: '800'}\n",
    "    ax1.yaxis.set_major_formatter(FuncFormatter(lambda y, _: label_ticks.get(y, '')))\n",
    "    ax1.set_ylabel('Dp (nm)')\n",
    "    ax1.set_title(f'{name} - {date}')\n",
    "\n",
    "    def log_tick_formatter(val, pos=None):\n",
    "        exponent = int(np.log10(val))\n",
    "        return f'$10^{{{exponent}}}$'\n",
    "\n",
    "    cbar = fig.colorbar(pcm, ax=ax1, pad=0.01, aspect=30)\n",
    "    cbar.set_label('$\\\\dfrac{dN}{dlogDp}$ (# cm$^{-3}$)')\n",
    "\n",
    "    cbar.ax.yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "\n",
    "    # --- Mean magnitude plot ---\n",
    "    ax2 = fig.add_subplot(gs[col_row + 1, col_col], sharex=ax1)\n",
    "    ax2.plot(df_day['local_time'], df_day['N_stp'], color='black', lw=2)\n",
    "    ax2.set_ylabel('Total N (# cm$^{-3}$)')\n",
    "\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%H'))\n",
    "    ax2.set_xlim(start, end)\n",
    "\n",
    "    ax2.set_xlabel('local_time')\n",
    "\n",
    "\n",
    "    # Align bottom plot width to top plot width\n",
    "    pos1 = ax1.get_position()\n",
    "    pos2 = ax2.get_position()\n",
    "    ax2.set_position([pos1.x0, pos2.y0, pos1.width, pos2.height])\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "filename = os.path.join(output_folder, f'pico_4_panel_event_plot_.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39d01a0c-164a-463d-a043-c8ec230a5e21",
   "metadata": {},
   "source": [
    "## Plot with time series with black carbon (30_min_avg) in bottom plot (1 figure per day)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf3368d7-fbb8-49c0-9f65-ecbd6a3fdf29",
   "metadata": {},
   "source": [
    "# --- Data setup ---\n",
    "dfp['local_time'] = pd.to_datetime(dfp['local_time'])\n",
    "dfp['date'] = dfp['local_time'].dt.date\n",
    "\n",
    "# Ensure dfp and SMPS are aligned\n",
    "assert dfp.shape[0] == dNdlogDp_stpp.shape[0], \"Mismatch between dfp and SMPS data!\"\n",
    "\n",
    "# Meshgrid setup (once, reused)\n",
    "Time = pd.to_datetime(dfp['local_time'])\n",
    "X = mdates.date2num(Time)\n",
    "Y = mid_Dp.copy()\n",
    "Z = dNdlogDp_stpp.copy().T  # Shape: (sizes, timestamps)\n",
    "Z_all = np.ma.masked_invalid(np.asarray(Z, dtype=np.float64))\n",
    "\n",
    "# --- Daily plot loop ---\n",
    "plot_counter = 0\n",
    "grouped = dfp.groupby('date')\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "for date, group in grouped:\n",
    "    # Mask for this date\n",
    "    day_mask = dfp['date'] == date\n",
    "    x_day = X[day_mask]\n",
    "    z_day = Z_all[:, day_mask]\n",
    "\n",
    "    if z_day.shape[1] == 0:\n",
    "        continue\n",
    "\n",
    "    black_carbon = dfp.loc[day_mask, '30_min_avg'].values\n",
    "\n",
    "    start = pd.to_datetime(str(date) + ' 00:00')\n",
    "    end = pd.to_datetime(str(date) + ' 23:59')\n",
    "\n",
    "    # --- Create figure with two panels ---\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0.05)\n",
    "\n",
    "    # --- Top: SMPS heatmap ---\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    XX, YY = np.meshgrid(x_day, Y)\n",
    "    pcm = ax1.pcolormesh(XX, YY[:, :z_day.shape[1]], z_day,\n",
    "                         shading='auto', cmap=custom_cmap,\n",
    "                         norm=LogNorm(vmin=10, vmax=1e5))\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylim(13, 800)\n",
    "    ax1.set_xlim(start, end)\n",
    "\n",
    "    yticks = [13, 20, 30, 40, 50, 60, 80, 100, 200, 300, 400, 500, 600, 700, 800]\n",
    "    ax1.set_yticks(yticks)\n",
    "    label_ticks = {13: '13', 100: '100', 800: '800'}\n",
    "    ax1.yaxis.set_major_formatter(FuncFormatter(lambda y, _: label_ticks.get(y, '')))\n",
    "    ax1.set_ylabel('Dp (nm)')\n",
    "    ax1.set_title(f'SMPS + Black Carbon - {date}')\n",
    "    ax1.tick_params(axis='x', labelbottom=False)\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = fig.colorbar(pcm, ax=ax1, pad=0.01, aspect=30)\n",
    "    cbar.set_label('$\\\\dfrac{dN}{dlogDp}$ (# cm$^{-3}$)')\n",
    "\n",
    "    # --- Bottom: Black Carbon ---\n",
    "    ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "    ax2.plot(dfp.loc[day_mask, 'local_time'], black_carbon, color='red', lw=2)\n",
    "    ax2.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    ax2.set_ylabel('Black Carbon\\n(µg/m³)')\n",
    "    ax2.set_xlim(start, end)\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%H'))\n",
    "    ax2.set_xlabel('local_time')\n",
    "\n",
    "    # --- Align subplot widths ---\n",
    "    pos1 = ax1.get_position()\n",
    "    pos2 = ax2.get_position()\n",
    "    ax2.set_position([pos1.x0, pos2.y0, pos1.width, pos2.height])\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "    filename = os.path.join(output_folder, f'time_series_plot_with_bc_{date}.png')\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    plot_counter += 1\n",
    "\n",
    "print(f'Number of days plotted: {plot_counter}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f44aeed-faea-40d2-a2f1-212dbc1d92dd",
   "metadata": {},
   "source": [
    "# Average N_stp over whole dataset (Weekend Versus Weekday)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cad5479c-c9b7-4d95-b07d-3351f04df919",
   "metadata": {},
   "source": [
    "# Move to other code, keep this here\n",
    "lower_p = dfp['N_stp'].quantile(0.01)\n",
    "upper_p = dfp['N_stp'].quantile(0.99)\n",
    "dfp_no_outliers = dfp[(dfp['N_stp'] >= lower_p) & (dfp['N_stp'] <= upper_p)].copy()\n",
    "\n",
    "# Make sure hour and dayofweek columns exist in the filtered dataframes\n",
    "for df in [dfp_no_outliers, dfp_no_outliers]:\n",
    "    df['local_time'] = pd.to_datetime(df['local_time'], errors='coerce')\n",
    "    df['hour'] = df['local_time'].dt.hour\n",
    "    df['dayofweek'] = df['local_time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    \n",
    "# Create weekday and weekend masks\n",
    "weekday_mask_p = dfp_no_outliers['dayofweek'].isin(range(5))  # 0-4\n",
    "weekend_mask_p = dfp_no_outliers['dayofweek'].isin([5,6])     # 5-6\n",
    "\n",
    "# Group by hour and take mean N_stp for weekdays and weekends\n",
    "weekday_p = dfp_no_outliers[weekday_mask_p].groupby('hour')['N_stp'].mean()\n",
    "weekend_p = dfp_no_outliers[weekend_mask_p].groupby('hour')['N_stp'].mean()\n",
    "\n",
    "#################\n",
    "# After filtering and adding 'hour' and 'dayofweek' columns (dfp_no_outliers and dfp_no_outliers)...\n",
    "\n",
    "# Print samples for Pico Rivera (dfp_no_outliers)\n",
    "print(\"Pico Rivera Sample Weekday Dates (with weekday name):\")\n",
    "for d in dfp_no_outliers[weekday_mask_p]['local_time'].dt.date.drop_duplicates().sort_values().head(10):\n",
    "    print(f\"{d} ({pd.to_datetime(d).strftime('%A')})\")\n",
    "\n",
    "print(\"\\nPico Rivera Sample Weekend Dates (with weekday name):\")\n",
    "for d in dfp_no_outliers[weekend_mask_p]['local_time'].dt.date.drop_duplicates().sort_values().head(10):\n",
    "    print(f\"{d} ({pd.to_datetime(d).strftime('%A')})\")\n",
    "\n",
    "#################\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(weekday_p.index, weekday_p, label='Pico Rivera - Weekday', color='tab:blue', linewidth=2)\n",
    "plt.plot(weekend_p.index, weekend_p, label='Pico Rivera - Weekend', color='tab:blue', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Mean N_stp (#/cm³)')\n",
    "\n",
    "plt.title('Weekday vs. Weekend Diurnal N_stp — Pico Rivera (Outliers Removed)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.xlim(0, 23)\n",
    "plt.xticks(range(0, 24))  # Show all hours 0 through 23\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, f'pico_weekend_vs_weekday_N_stp_.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "317e632e-9e65-4898-8b86-41b3e6407fc7",
   "metadata": {},
   "source": [
    "## Average N_stp and BC (30_min_avg) over whole dataset (Weekend Versus Weekday)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f195dbd-a980-4f9a-b304-9a47028fdf17",
   "metadata": {},
   "source": [
    "# --- 1️⃣ Function to remove outliers ---\n",
    "def remove_outliers(df, column):\n",
    "    lower = df[column].quantile(0.01)\n",
    "    upper = df[column].quantile(0.99)\n",
    "    return df[(df[column] >= lower) & (df[column] <= upper)].copy()\n",
    "\n",
    "# --- 2️⃣ Remove outliers for N_stp and BC ---\n",
    "df_nstp = remove_outliers(dfp, 'N_stp')\n",
    "df_bc = remove_outliers(dfp, '30_min_avg')\n",
    "\n",
    "# --- 3️⃣ Convert time column to datetime and extract hour/dayofweek ---\n",
    "for df in [df_nstp, df_bc]:\n",
    "    df['local_time'] = pd.to_datetime(df['local_time'], errors='coerce')\n",
    "    df['hour'] = df['local_time'].dt.hour\n",
    "    df['dayofweek'] = df['local_time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# --- 4️⃣ Weekday/weekend masks ---\n",
    "weekday_mask_n = df_nstp['dayofweek'].isin(range(5))\n",
    "weekend_mask_n = df_nstp['dayofweek'].isin([5,6])\n",
    "\n",
    "weekday_mask_bc = df_bc['dayofweek'].isin(range(5))\n",
    "weekend_mask_bc = df_bc['dayofweek'].isin([5,6])\n",
    "\n",
    "# --- 5️⃣ Compute hourly means ---\n",
    "weekday_nstp = df_nstp[weekday_mask_n].groupby('hour')['N_stp'].mean()\n",
    "weekend_nstp = df_nstp[weekend_mask_n].groupby('hour')['N_stp'].mean()\n",
    "\n",
    "weekday_bc = df_bc[weekday_mask_bc].groupby('hour')['30_min_avg'].mean()\n",
    "weekend_bc = df_bc[weekend_mask_bc].groupby('hour')['30_min_avg'].mean()\n",
    "\n",
    "# --- 6️⃣ Plot both variables in stacked subplots ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# N_stp Plot\n",
    "axes[0].plot(weekday_nstp.index, weekday_nstp, label='Weekday', color='tab:blue', linewidth=2)\n",
    "axes[0].plot(weekend_nstp.index, weekend_nstp, label='Weekend', color='tab:blue', linestyle='--', linewidth=2)\n",
    "axes[0].set_ylabel('Mean N_stp (#/cm³)')\n",
    "\n",
    "axes[0].set_title('Weekday vs. Weekend Diurnal N_stp — Pico Rivera (Outliers Removed)')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# BC Plot\n",
    "axes[1].plot(weekday_bc.index, weekday_bc, label='Weekday', color='tab:red', linewidth=2)\n",
    "axes[1].plot(weekend_bc.index, weekend_bc, label='Weekend', color='tab:red', linestyle='--', linewidth=2)\n",
    "axes[1].set_ylabel('Mean BC (µg/m³)')\n",
    "axes[1].set_xlabel('Hour of Day')\n",
    "axes[1].set_title('Weekday vs. Weekend Diurnal BC — Pico Rivera (Outliers Removed)')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.xlim(0, 23)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, f'pico_weekend_vs_weekday_N_stp_BC_.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a274dc5d-05e9-48e8-9cfa-774a11c8cef5",
   "metadata": {},
   "source": [
    "# Event Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217141f1-edd4-44d4-b431-ebbc7dd7ff28",
   "metadata": {},
   "source": [
    "- These are estimated frequencies, you will need to define your own how you wish\n",
    "\n",
    "- Feel free to change the frequencies here and even the variable name from pico_freq, just be sure to change the varable name from pico_freq to the name you picked in this line: bars1 = ax.bar(x, pico_freq, width, label='Pico Rivera', color='#1f77b4')  \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1794fb7-92da-41f1-a7d5-d9331d3f4043",
   "metadata": {},
   "source": [
    "# Example data\n",
    "event_types = ['Daytime', 'Nighttime', 'Non-event'] # these can have different names\n",
    "pico_freq = [0.13333333333333333, 0.45555555555555555, 0.45555555555555555] # change the frequencies\n",
    "x = np.arange(len(event_types))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars1 = ax.bar(x, pico_freq, width, label='Pico Rivera', color='#1f77b4')\n",
    "\n",
    "# Labels and styling\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Event Type Frequencies')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(event_types)\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.5, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, f'pico_event_frequencies.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78798be0-da06-4bc3-9148-ba725d158e8b",
   "metadata": {},
   "source": [
    "# Average Particle size distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d53cec-3b11-4e44-b417-576f6d3d8fb5",
   "metadata": {},
   "source": [
    "Calculate the average size distributions during the day (9am to 8pm) and night (9pm to 8am) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082e5c4-5aaa-49ff-b8cb-70cf52739f5d",
   "metadata": {},
   "source": [
    "Divide the dataset into day and night, and then calculate the average size distributions during those time periods for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "63929418-906c-45b4-946e-4b8a6db7ea97",
   "metadata": {},
   "source": [
    "df_copyp = tsdfp.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8009fbdb-f62e-470c-b13e-e6ade2f40359",
   "metadata": {},
   "source": [
    "\n",
    "df_copyp.columns[concentration_columns]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb469066-0d19-435d-93ab-8f5d6421f53c",
   "metadata": {},
   "source": [
    "## Start with the taking the average dNdlogDp across all times for Pico Rivera and Rubidoux"
   ]
  },
  {
   "cell_type": "code",
   "id": "5671e945-028a-4273-b194-5d2703a37ddb",
   "metadata": {},
   "source": [
    "# Change columns here if needed\n",
    "selected_colsp = df_copyp.columns[concentration_columns]\n",
    "bin_diametersp = np.array(selected_colsp).astype(float)\n",
    "dNdlogDp_stpp_numeric = np.array(dNdlogDp_stpp, dtype='float')\n",
    "# Mask out bins with all NaNs\n",
    "valid_binsp = ~np.all(np.isnan(dNdlogDp_stpp_numeric), axis=0)\n",
    "\n",
    "# Compute average across time for valid bins\n",
    "mean_spectrum_p = np.nanmean(dNdlogDp_stpp_numeric[:, valid_binsp], axis=0)\n",
    "bin_diameters_validp = bin_diametersp[valid_binsp]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "982c1d33-b102-48ad-a6bd-1f9a36bff32b",
   "metadata": {},
   "source": [
    "## Calculate the average size distributions during the day (9am to 8pm) and night (9pm to 8am) "
   ]
  },
  {
   "cell_type": "code",
   "id": "8b76a4ba-028a-4a53-9032-f3b48ff83c5f",
   "metadata": {},
   "source": [
    "df_copyp = df_copyp.reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a9fe500-41a0-41fa-a9ed-e0f07f3bd830",
   "metadata": {},
   "source": [
    "df_copyp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64f29aa3-7755-4bef-810d-7a79d5472a5e",
   "metadata": {},
   "source": [
    "# Convert timestamp column to datetime, if not already\n",
    "df_copyp['local_time'] = pd.to_datetime(df_copyp['local_time'])\n",
    "\n",
    "# Extract hour\n",
    "df_copyp['hour'] = df_copyp['local_time'].dt.hour"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d90f280e-fa80-42c9-a4e1-756a544995a3",
   "metadata": {},
   "source": [
    "# Daytime: 09:00–20:00\n",
    "day_maskp = df_copyp['hour'].between(9, 20)\n",
    "\n",
    "# Nighttime: 21:00–23:59 and 00:00–08:59\n",
    "night_maskp = ~day_maskp  # everything else"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f13489be-bac5-49bb-b680-b9d5628cb57a",
   "metadata": {},
   "source": [
    "# Convert dNdlogDp_stpp to numeric if not already done\n",
    "dNdlogDp_stpp_numeric = pd.DataFrame(dNdlogDp_stpp).apply(pd.to_numeric, errors='coerce').values\n",
    "\n",
    "# Apply masks to get subsets\n",
    "dNdlogDp_dayp = dNdlogDp_stpp_numeric[day_maskp.values]\n",
    "dNdlogDp_nightp = dNdlogDp_stpp_numeric[night_maskp.values]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d5e2d87-4034-4c0b-94bd-c82b9d5aa844",
   "metadata": {},
   "source": [
    "mean_dayp = np.nanmean(dNdlogDp_dayp, axis=0)\n",
    "mean_nightp = np.nanmean(dNdlogDp_nightp, axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03fe5e11-9c65-4d36-a08b-42318ba50b0d",
   "metadata": {},
   "source": [
    "## Plot the total and day/night averages on same figure"
   ]
  },
  {
   "cell_type": "code",
   "id": "b41474e6-982a-4672-b360-9e985f2dbe6f",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)  # 1 row, 2 cols, share y-axis\n",
    "\n",
    "# First subplot: Average size distribution comparison\n",
    "axes[0].plot(bin_diameters_validp, mean_spectrum_p, marker='o', label='Pico Rivera', color='blue')\n",
    "\n",
    "\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Particle Diameter (nm)')\n",
    "axes[0].set_ylabel('Mean dN/dlogDp (#/cm³)')\n",
    "axes[0].set_title('Average Particle Size Distribution')\n",
    "\n",
    "axes[0].tick_params(axis='both', which='major')\n",
    "\n",
    "axes[0].grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xticks([10, 100])\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "# Second subplot: Day vs Night for both sites\n",
    "axes[1].plot(bin_diameters_validp, mean_dayp, label='Pico Day (9am–8pm)', color='blue', linewidth=3)\n",
    "axes[1].plot(bin_diameters_validp, mean_nightp, label='Pico Night (9pm–8am)', color='blue', linestyle='--', linewidth=3)\n",
    "\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Particle Diameter (nm)')\n",
    "axes[1].set_title('Day vs Night Particle Size Distribution')\n",
    "axes[1].tick_params(axis='both', which='major')\n",
    "\n",
    "axes[1].grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, f'pico_day_vs_night_dNdlogDp.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e96f5592-60f3-45c9-8ab9-be09a748b2b0",
   "metadata": {},
   "source": [
    "## Average Total Number Concentration per Weekday"
   ]
  },
  {
   "cell_type": "code",
   "id": "e8c6966f-9191-4623-9ce4-a3e6b5dd731e",
   "metadata": {},
   "source": [
    "dfp['local_time'] = pd.to_datetime(dfp['local_time'], errors='coerce')\n",
    "dfp = dfp.set_index('local_time')\n",
    "dfp = dfp.dropna(subset=['N_stp'])\n",
    "dfp['weekday'] = dfp.index.day_name()\n",
    "\n",
    "# Remove outliers using z-score\n",
    "dfp_copy = dfp.copy()\n",
    "dfp_copy = dfp_copy[np.abs(zscore(dfp_copy['N_stp'].astype(float), nan_policy='omit')) < 3]\n",
    "\n",
    "# Define ordered weekdays\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Create a list of (weekday, DataFrame) tuples in order\n",
    "weekday_groups = [(day, dfp_copy[dfp_copy['weekday'] == day]) for day in ordered_days]\n",
    "\n",
    "# Plot setup\n",
    "fig, axs = plt.subplots(1, 7, figsize=(18, 5), sharey=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (day, group) in enumerate(weekday_groups):\n",
    "    avg_diurnal = group.groupby(group.index.hour)['N_stp'].mean()\n",
    "    axs[i].plot(avg_diurnal.index, avg_diurnal.values)\n",
    "    axs[i].set_title(day)\n",
    "    axs[i].set_xlabel('Hour of Day')\n",
    "\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Avg N_stp (# cm$^{-3}$)')\n",
    "\n",
    "# Final plot formatting\n",
    "plt.suptitle('Average Total Number Concentration per Weekday (Pico Rivera)')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "filename = os.path.join(output_folder, f'pico_avg_N_stp_per_weekday.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0196d29-4b1b-4893-aa0a-8e2aaefd3bf7",
   "metadata": {},
   "source": [
    "## N_stp averaged and BC over days of the week for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "7873607b-2c47-4d0d-9c7b-3df04162f812",
   "metadata": {},
   "source": [
    "# --- 1️⃣ Copy dfp to avoid SettingWithCopy warnings ---\n",
    "\n",
    "# --- 2️⃣ Remove outliers using z-score ---\n",
    "dfp_copy = dfp_copy[np.abs(zscore(dfp_copy['N_stp'].astype(float), nan_policy='omit')) < 3]\n",
    "if '30_min_avg' in dfp_copy.columns:\n",
    "    dfp_copy = dfp_copy[np.abs(zscore(dfp_copy['30_min_avg'].astype(float), nan_policy='omit')) < 3]\n",
    "\n",
    "# --- 3️⃣ Ensure datetime index and add weekday ---\n",
    "if not isinstance(dfp_copy.index, pd.DatetimeIndex):\n",
    "    dfp_copy.index = pd.to_datetime(dfp_copy.index, errors='coerce')\n",
    "\n",
    "dfp_copy['weekday'] = dfp_copy.index.day_name()\n",
    "\n",
    "# --- 4️⃣ Define weekday order ---\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# --- 5️⃣ Create weekday groups ---\n",
    "weekday_groups = [(day, dfp_copy[dfp_copy['weekday'] == day]) for day in ordered_days]\n",
    "\n",
    "# --- 6️⃣ Plot setup: 2 rows, shared y per row ---\n",
    "fig, axs = plt.subplots(2, 7, figsize=(20, 8), sharex=True, sharey='row')\n",
    "axs = axs.reshape(2, 7)\n",
    "\n",
    "hours = range(24)  # x-axis for hours\n",
    "\n",
    "# --- Row 1: N_stp ---\n",
    "for i, (day, group) in enumerate(weekday_groups):\n",
    "    if not group.empty:\n",
    "        avg_diurnal = group.groupby(group.index.hour)['N_stp'].mean()\n",
    "        axs[0, i].plot(avg_diurnal.index, avg_diurnal.values, color='tab:blue', linewidth=2)\n",
    "    axs[0, i].set_title(day)\n",
    "    axs[0, i].set_xlim(0, 23)\n",
    "    axs[0, i].set_xticks(range(0, 24, 4))\n",
    "    if i == 0:\n",
    "        axs[0, i].set_ylabel('Avg N_stp (# cm$^{-3}$)')\n",
    "    axs[0, i].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Optional: set a reasonable y-limit for N_stp\n",
    "axs[0, 0].set_ylim(0, dfp_copy['N_stp'].max() * 1.1)\n",
    "\n",
    "# --- Row 2: 30_min_avg ---\n",
    "for i, (day, group) in enumerate(weekday_groups):\n",
    "    if '30_min_avg' in group.columns and not group.empty:\n",
    "        avg_diurnal_bc = group.groupby(group.index.hour)['30_min_avg'].mean()\n",
    "        axs[1, i].plot(avg_diurnal_bc.index, avg_diurnal_bc.values, color='tab:red', linewidth=2)\n",
    "    axs[1, i].set_xlim(0, 23)\n",
    "    axs[1, i].set_xticks(range(0, 24, 4))\n",
    "    if i == 0:\n",
    "        axs[1, i].set_ylabel('Avg BC (µg/m³)')\n",
    "    axs[1, i].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Optional: set a reasonable y-limit for BC\n",
    "axs[1, 0].set_ylim(0, dfp_copy['30_min_avg'].max() * 1.1)\n",
    "\n",
    "# --- 7️⃣ Final formatting ---\n",
    "for ax in axs[1, :]:\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "\n",
    "fig.suptitle('Average Diurnal Profiles per Weekday (Pico Rivera, Outliers Removed via Z-score)')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "filename = os.path.join(output_folder, 'pico_avg_N_stp_BC_per_weekday.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39336fbb-e502-4d97-a538-e1f4438d1f58",
   "metadata": {},
   "source": [
    "## Average dNdlogDp per Hour"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c15ff9e-db31-4657-bac6-ead1d1827c40",
   "metadata": {},
   "source": [
    "# Convert to float array\n",
    "dNdlogDp_stpp = np.asarray(dNdlogDp_stpp, dtype=float)\n",
    "\n",
    "# Step 1: Calculate z-scores across columns (bins)\n",
    "z_scores = np.abs(zscore(dNdlogDp_stpp, axis=0, nan_policy='omit'))\n",
    "\n",
    "# Step 2: Keep only rows where all bins are < 3 std dev\n",
    "outlier_mask = (z_scores < 3).all(axis=1)\n",
    "\n",
    "# Step 3: Filter both the data and the datetime index\n",
    "dNdlogDp_filtered = dNdlogDp_stpp[outlier_mask]\n",
    "dfp_filtered = tsdfp[outlier_mask]\n",
    "\n",
    "# Step 4: Add hour column and group by hour\n",
    "dfp_filtered.index = pd.to_datetime(dfp_filtered.index)\n",
    "dfp_filtered['hour'] = dfp_filtered.index.hour\n",
    "\n",
    "df_number = pd.DataFrame(dNdlogDp_filtered, index=dfp_filtered['hour'])\n",
    "hourly_avgp = df_number.groupby(df_number.index).mean()  # shape: (24, n_bins)\n",
    "\n",
    "# Step 5: Plot with turbo colormap\n",
    "cmap = cm.get_cmap('turbo')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for hr in range(24):\n",
    "    color = cmap(hr / 23)  # normalize hour → [0, 1]\n",
    "    plt.plot(bin_diametersp, hourly_avgp.iloc[hr], label=f'{hr:02d}', color=color)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Particle Diameter (nm)')\n",
    "plt.ylabel('Mean $\\\\dfrac{dN}{dlogDp}$ (# cm$^{-3}$)')\n",
    "plt.title('Pico Rivera Hourly Average Number Size Distributions (Outliers Removed)')\n",
    "plt.legend(ncol=3, title='Hour')\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, f'pico_avg_dNdlogDp_per_hour.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73895c42-86c5-406a-9683-5e985f162a9d",
   "metadata": {},
   "source": [
    "# Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e1f3c0c-3daa-40ff-8571-73f69e1d26ae",
   "metadata": {},
   "source": [
    "dfp = dfp.reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4c35aea-4d10-4c37-b4e7-36a5a435d55e",
   "metadata": {},
   "source": [
    "# Growth Rate Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d136013-3a0a-4b11-b581-06ecefcc3762",
   "metadata": {},
   "source": [
    "Preliminary, calculates GR in nm/hr using the geometric mean diameter (Geo. Mean (nm)):\n",
    "\n",
    "[Geo. Mean (nm) at Time 2 - Geo. Mean (nm) at Time 1] / [Time 2 - Time 1]"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5048931-ddea-41d1-802e-26264ae62fe8",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3c796a0-8229-4a5b-b151-6afe1f07dfd4",
   "metadata": {},
   "source": [
    "def add_growth_rate_column(dfp, time_col='local_time', diam_col='Geo. Mean (nm)', max_gap_min=3):\n",
    "    \"\"\"\n",
    "    Adds a growth rate column (nm/hr) to the DataFrame by computing\n",
    "    the slope between each point and the next valid point (gap ≤ max_gap_min).\n",
    "    \"\"\"\n",
    "    df = dfp.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    \n",
    "    # Initialize growth rate column\n",
    "    growth_rates = np.full(len(df), np.nan)\n",
    "    \n",
    "    for i in range(len(df) - 1):\n",
    "        dt = (df[time_col].iloc[i+1] - df[time_col].iloc[i]).total_seconds() / 60  # in minutes\n",
    "        if dt <= max_gap_min and dt > 0:\n",
    "            dD = df[diam_col].iloc[i+1] - df[diam_col].iloc[i]  # diameter change (nm)\n",
    "            growth_rates[i] = dD / (dt / 60)  # convert to nm/hr\n",
    "    \n",
    "    df['growth_rate_nm_hr'] = growth_rates\n",
    "    return df\n",
    "\n",
    "# --- Example usage ---\n",
    "dfp = add_growth_rate_column(dfp, time_col='local_time', diam_col='Geo. Mean (nm)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0133721-30df-4c56-9531-9bef3d28fe94",
   "metadata": {},
   "source": [
    "## Average N_stp, BC, and GR in June, July"
   ]
  },
  {
   "cell_type": "code",
   "id": "408071ee-de1f-4979-811a-0f6edcab79f8",
   "metadata": {},
   "source": [
    "# --- 1️⃣ Function to remove outliers ---\n",
    "def remove_outliers(df, column):\n",
    "    lower = df[column].quantile(0.01)\n",
    "    upper = df[column].quantile(0.99)\n",
    "    return df[(df[column] >= lower) & (df[column] <= upper)].copy()\n",
    "\n",
    "# --- 2️⃣ Remove outliers for N_stp, BC, and GR ---\n",
    "df_nstp = remove_outliers(dfp, 'N_stp')\n",
    "df_bc = remove_outliers(dfp, '30_min_avg')\n",
    "df_gr = remove_outliers(dfp, 'growth_rate_nm_hr')\n",
    "\n",
    "# --- 3️⃣ Convert time column to datetime and extract hour/dayofweek/month ---\n",
    "for df in [df_nstp, df_bc, df_gr]:\n",
    "    df['local_time'] = pd.to_datetime(df['local_time'], errors='coerce')\n",
    "    df['hour'] = df['local_time'].dt.hour\n",
    "    df['dayofweek'] = df['local_time'].dt.dayofweek\n",
    "    df['month'] = df['local_time'].dt.month\n",
    "\n",
    "# --- 4️⃣ Define months of interest ---\n",
    "months = {6: 'June', 7: 'July'}\n",
    "\n",
    "# --- 5️⃣ Create figure with 3 rows (N_stp, BC, GR) × 2 columns (months) ---\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 12), sharex=True)\n",
    "\n",
    "for idx, (month_num, month_name) in enumerate(months.items()):\n",
    "    # --- Weekday/weekend masks ---\n",
    "    weekday_n = (df_nstp['month'] == month_num) & df_nstp['dayofweek'].isin(range(5))\n",
    "    weekend_n = (df_nstp['month'] == month_num) & df_nstp['dayofweek'].isin([5,6])\n",
    "    \n",
    "    weekday_bc = (df_bc['month'] == month_num) & df_bc['dayofweek'].isin(range(5))\n",
    "    weekend_bc = (df_bc['month'] == month_num) & df_bc['dayofweek'].isin([5,6])\n",
    "    \n",
    "    weekday_gr = (df_gr['month'] == month_num) & df_gr['dayofweek'].isin(range(5))\n",
    "    weekend_gr = (df_gr['month'] == month_num) & df_gr['dayofweek'].isin([5,6])\n",
    "    \n",
    "    # --- Hourly means ---\n",
    "    weekday_nstp = df_nstp[weekday_n].groupby('hour')['N_stp'].mean()\n",
    "    weekend_nstp = df_nstp[weekend_n].groupby('hour')['N_stp'].mean()\n",
    "    \n",
    "    weekday_bc_data = df_bc[weekday_bc].groupby('hour')['30_min_avg'].mean()\n",
    "    weekend_bc_data = df_bc[weekend_bc].groupby('hour')['30_min_avg'].mean()\n",
    "    \n",
    "    weekday_gr_data = df_gr[weekday_gr].groupby('hour')['growth_rate_nm_hr'].mean()\n",
    "    weekend_gr_data = df_gr[weekend_gr].groupby('hour')['growth_rate_nm_hr'].mean()\n",
    "\n",
    "    # --- Plot N_stp ---\n",
    "    axes[0, idx].plot(weekday_nstp.index, weekday_nstp, label='Weekday', color='tab:blue')\n",
    "    axes[0, idx].plot(weekend_nstp.index, weekend_nstp, label='Weekend', color='tab:blue', linestyle='--')\n",
    "    axes[0, idx].set_title(f'{month_name} – N_stp')\n",
    "    axes[0, idx].grid(True, linestyle='--', alpha=0.3)\n",
    "    if idx == 0:\n",
    "        axes[0, idx].set_ylabel('Mean N_stp (#/cm³)')\n",
    "        axes[0, idx].legend()\n",
    "\n",
    "    # --- Plot BC ---\n",
    "    axes[1, idx].plot(weekday_bc_data.index, weekday_bc_data, label='Weekday', color='tab:red')\n",
    "    axes[1, idx].plot(weekend_bc_data.index, weekend_bc_data, label='Weekend', color='tab:red', linestyle='--')\n",
    "    axes[1, idx].set_title(f'{month_name} – BC')\n",
    "    axes[1, idx].grid(True, linestyle='--', alpha=0.3)\n",
    "    if idx == 0:\n",
    "        axes[1, idx].set_ylabel('Mean BC (µg/m³)')\n",
    "        axes[1, idx].legend()\n",
    "\n",
    "    # --- Plot Growth Rate ---\n",
    "    axes[2, idx].plot(weekday_gr_data.index, weekday_gr_data, label='Weekday', color='tab:green')\n",
    "    axes[2, idx].plot(weekend_gr_data.index, weekend_gr_data, label='Weekend', color='tab:green', linestyle='--')\n",
    "    axes[2, idx].set_title(f'{month_name} – Growth Rate')\n",
    "    axes[2, idx].grid(True, linestyle='--', alpha=0.3)\n",
    "    if idx == 0:\n",
    "        axes[2, idx].set_ylabel('Growth Rate (nm/hr)')\n",
    "        axes[2, idx].legend()\n",
    "\n",
    "# --- Shared X-axis settings ---\n",
    "for ax in axes[2]:\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.set_xlim(0, 23)\n",
    "        ax.set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, f'pico_weekend_vs_weekday_monthly.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8da9a68e-5afe-4af0-a062-8959fe47e9d7",
   "metadata": {},
   "source": [
    "dfp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a0b9387-6bae-43f7-b33e-0d088cbae7ec",
   "metadata": {},
   "source": [
    "## Colormap time series plot of dNdlogDp_stp over the whole time period with the new, warped colormap"
   ]
  },
  {
   "cell_type": "code",
   "id": "980c9027-a986-4ed9-b230-2f6311096d14",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d16bc4b-75b3-4b47-a6e4-be56e954190a",
   "metadata": {},
   "source": [
    "dNdlogDp_stpp.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b086b485-e1f3-4092-9689-9b93979dd614",
   "metadata": {},
   "source": [
    "# ---- Custom colormap warp ----\n",
    "def warp_colormap(cmap, warp_func, N=256):\n",
    "    orig = np.linspace(0, 1, N)\n",
    "    warped = warp_func(orig)\n",
    "    warped = np.clip(warped, 0, 1)\n",
    "    warped_colors = cmap(warped)\n",
    "    return LinearSegmentedColormap.from_list(\"warped_cmap\", warped_colors)\n",
    "\n",
    "def high_end_bias(x):\n",
    "    return x**3\n",
    "\n",
    "# Base colors (your custom palette)\n",
    "colors = ['#313695', '#4575b4', '#74add1', '#a6cfe2', '#fddcaa',\n",
    "          '#fdae61', '#f46d43', '#d73027', '#a50026']\n",
    "base_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "# Warp it\n",
    "custom_cmap = warp_colormap(base_cmap, high_end_bias)\n",
    "\n",
    "Time = pd.to_datetime(dfp['local_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "X = mdates.date2num(Time.to_numpy())\n",
    "\n",
    "Y = mid_Dp  # shape (115,)\n",
    "\n",
    "Z = np.asarray(dNdlogDp_stpp.T, dtype=np.float64)  # shape should be (115, len(X))\n",
    "\n",
    "if Z.shape[1] != len(X):\n",
    "    raise ValueError(f\"Mismatch: Z has {Z.shape[1]} columns, X has {len(X)} timestamps\")\n",
    "\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "masked_Z = np.ma.masked_invalid(Z)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "vmin = 10\n",
    "vmax = 1e5\n",
    "\n",
    "pcm = ax1.pcolormesh(\n",
    "    XX, YY, masked_Z,\n",
    "    shading='auto',\n",
    "    cmap=custom_cmap,\n",
    "    norm=LogNorm(vmin=vmin, vmax=vmax)\n",
    ")\n",
    "\n",
    "# Colorbar with log ticks\n",
    "def log_tick_formatter(val, pos=None):\n",
    "    exponent = int(np.log10(val))\n",
    "    return f'$10^{{{exponent}}}$'\n",
    "\n",
    "cbar = fig.colorbar(pcm, ax=ax1, label='$\\\\dfrac{dN}{dlogDp}$ (stp cts cm$^{-3}$)')\n",
    "cbar.ax.yaxis.set_major_formatter(ticker.FuncFormatter(log_tick_formatter))\n",
    "\n",
    "cbar.set_label(cbar.ax.get_ylabel())\n",
    "\n",
    "\n",
    "# Axis formatting\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylim(10, max(Y))\n",
    "ax1.set_ylabel('Dp (nm)')\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "\n",
    "\n",
    "# X-axis ticks every 15 days\n",
    "ax1.xaxis.set_major_locator(mdates.DayLocator(interval=15))\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# Title\n",
    "ax1.set_title('Pico Rivera Size Distribution Time Series')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, 'pico_size_dist_time_series_warped_colormap.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8e1d25f-27f7-42b4-b9b1-3f6eac6bdedf",
   "metadata": {},
   "source": [
    "## Same Plot but for longer time periods:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a795672a-fad2-43bb-8ac4-28f0d39c0b95",
   "metadata": {},
   "source": [
    "# ---- Custom colormap warp ----\n",
    "def warp_colormap(cmap, warp_func, N=256):\n",
    "    orig = np.linspace(0, 1, N)\n",
    "    warped = warp_func(orig)\n",
    "    warped = np.clip(warped, 0, 1)\n",
    "    warped_colors = cmap(warped)\n",
    "    return LinearSegmentedColormap.from_list(\"warped_cmap\", warped_colors)\n",
    "\n",
    "def high_end_bias(x):\n",
    "    return x**3\n",
    "\n",
    "# Base colors (your custom palette)\n",
    "colors = ['#313695', '#4575b4', '#74add1', '#a6cfe2', '#fddcaa',\n",
    "          '#fdae61', '#f46d43', '#d73027', '#a50026']\n",
    "base_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "# Warp it\n",
    "custom_cmap = warp_colormap(base_cmap, high_end_bias)\n",
    "\n",
    "Time = pd.to_datetime(dfp['local_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "X = mdates.date2num(Time.to_numpy())\n",
    "\n",
    "Y = mid_Dp  # shape (115,)\n",
    "\n",
    "Z = np.asarray(dNdlogDp_stpp.T, dtype=np.float64)  # shape should be (115, len(X))\n",
    "\n",
    "if Z.shape[1] != len(X):\n",
    "    raise ValueError(f\"Mismatch: Z has {Z.shape[1]} columns, X has {len(X)} timestamps\")\n",
    "\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "masked_Z = np.ma.masked_invalid(Z)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "vmin = 10\n",
    "vmax = 1e5\n",
    "\n",
    "pcm = ax1.pcolormesh(\n",
    "    XX, YY, masked_Z,\n",
    "    shading='auto',\n",
    "    cmap=custom_cmap,\n",
    "    norm=LogNorm(vmin=vmin, vmax=vmax)\n",
    ")\n",
    "\n",
    "# Colorbar with log ticks\n",
    "def log_tick_formatter(val, pos=None):\n",
    "    exponent = int(np.log10(val))\n",
    "    return f'$10^{{{exponent}}}$'\n",
    "\n",
    "cbar = fig.colorbar(pcm, ax=ax1, label='$\\\\dfrac{dN}{dlogDp}$ (stp cts cm$^{-3}$)')\n",
    "cbar.ax.yaxis.set_major_formatter(ticker.FuncFormatter(log_tick_formatter))\n",
    "\n",
    "cbar.set_label(cbar.ax.get_ylabel())\n",
    "\n",
    "\n",
    "# Axis formatting\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylim(10, max(Y))\n",
    "ax1.set_ylabel('Dp (nm)')\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "\n",
    "\n",
    "# X-axis ticks every 60 days\n",
    "ax1.xaxis.set_major_locator(mdates.DayLocator(interval=60))\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# Title\n",
    "ax1.set_title('Pico Rivera Size Distribution Time Series')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, 'pico_size_dist_time_series_warped_colormap.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "617a33ec-cd57-4136-8dc5-66ca1ccdbe40",
   "metadata": {},
   "source": [
    "## Colormap time series plot of dNdlogDp_stp over the whole time period with the old colormap"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6e1c04e-11a6-496a-b954-47a013dda94a",
   "metadata": {},
   "source": [
    "# Base colors (your custom palette)\n",
    "colors = ['#313695', '#4575b4', '#74add1', '#a6cfe2', '#fddcaa',\n",
    "          '#fdae61', '#f46d43', '#d73027', '#a50026']\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "# ---- Prepare data for pcolormesh ----\n",
    "Time = pd.to_datetime(dfp['local_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "X = mdates.date2num(Time)           # numeric time for pcolormesh\n",
    "Y = mid_Dp.copy()                   # particle diameters\n",
    "XX, YY = np.meshgrid(X, Y)          # 2D mesh\n",
    "Z = np.asarray(dNdlogDp_stpp.T, dtype=np.float64)  # shape must be (len(Y), len(X))\n",
    "masked_Z = np.ma.masked_invalid(Z)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "vmin = 10\n",
    "vmax = 1e5\n",
    "\n",
    "pcm = ax1.pcolormesh(\n",
    "    XX, YY, masked_Z,\n",
    "    shading='auto',\n",
    "    cmap=custom_cmap,\n",
    "    norm=LogNorm(vmin=vmin, vmax=vmax)\n",
    ")\n",
    "\n",
    "# Colorbar with log ticks\n",
    "def log_tick_formatter(val, pos=None):\n",
    "    exponent = int(np.log10(val))\n",
    "    return f'$10^{{{exponent}}}$'\n",
    "\n",
    "cbar = fig.colorbar(pcm, ax=ax1, label='$\\\\dfrac{dN}{dlogDp}$ (stp cts cm$^{-3}$)')\n",
    "cbar.ax.yaxis.set_major_formatter(ticker.FuncFormatter(log_tick_formatter))\n",
    "\n",
    "cbar.set_label(cbar.ax.get_ylabel())\n",
    "\n",
    "# Axis formatting\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylim(10, max(Y))\n",
    "ax1.set_ylabel('Dp (nm)')\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "\n",
    "# X-axis ticks every 15 days\n",
    "ax1.xaxis.set_major_locator(mdates.DayLocator(interval=15))\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# Title\n",
    "ax1.set_title('Pico Rivera Size Distribution Time Series')\n",
    "\n",
    "plt.tight_layout()\n",
    "filename = os.path.join(output_folder, 'pico_size_dist_time_series_old_colormap.png')\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "debd6b4f-3d16-4d8a-9ea7-5b09aa8dd849",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1_ese101",
   "language": "python",
   "name": "hw1_ese101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
